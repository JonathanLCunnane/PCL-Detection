{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0000001",
   "metadata": {},
   "source": [
    "# Experiment A: Keyword One-Hot Feature (`FeatureType.KEYWORD`)\n",
    "\n",
    "Augment DeBERTa-v3-base with a one-hot encoding of the `keyword` metadata field\n",
    "using the `FeatureType.KEYWORD` / `extract_keyword_feature` framework from `utils/feature.py`.\n",
    "\n",
    "The keyword identifies which vulnerable community the article discusses\n",
    "(e.g. refugee, homeless, disabled, women, children…) — information not\n",
    "always present verbatim in the text.\n",
    "\n",
    "Keyword features are built per-split via `make_keyword_dataloaders` so the\n",
    "`keyword_to_idx` vocabulary (and therefore feature dimension) is derived\n",
    "from `train_sub_df` only, preventing leakage.\n",
    "\n",
    "Feature combination (CONCAT / GMF) is searched as a hyperparameter.\n",
    "\n",
    "**Fixed corrections:** VAL_FRACTION=0.15, BATCH_SIZE=32, NUM_EPOCHS=12,\n",
    "PATIENCE=4, pooling searched over all 4 strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import logging\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.metrics import classification_report\n",
    "import optuna\n",
    "from optuna.visualization.matplotlib import (\n",
    "    plot_optimization_history,\n",
    "    plot_param_importances,\n",
    "    plot_parallel_coordinate,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "from utils.data import load_data_with_keyword\n",
    "from utils.split import split_train_val\n",
    "from utils.dataloaders import make_keyword_dataloaders\n",
    "from utils.feature import FeatureType, extract_keyword_feature\n",
    "from utils.pcl_deberta import PCLDeBERTa, PoolingStrategy\n",
    "from utils.feature_comb import FeatureComb\n",
    "from utils.optim import compute_pos_weight\n",
    "from utils.training_loop import train_model\n",
    "from utils.eval import evaluate\n",
    "\n",
    "SEED = 42\n",
    "DATA_DIR = \"../data\"\n",
    "OUT_DIR = \"out\"\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "MAX_LENGTH = 256\n",
    "VAL_FRACTION = 0.15\n",
    "BATCH_SIZE = 32\n",
    "N_TRIALS = 20\n",
    "NUM_EPOCHS = 12\n",
    "PATIENCE = 4\n",
    "N_EVAL_STEPS = 35\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s:\\t%(message)s\")\n",
    "LOG = logging.getLogger(__name__)\n",
    "LOG.info(f\"Device: {DEVICE}\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0000003",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, dev_df = load_data_with_keyword(DATA_DIR)\n",
    "train_sub_df, val_sub_df = split_train_val(train_df, val_frac=VAL_FRACTION, seed=SEED)\n",
    "tokeniser = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "LOG.info(f\"Train: {len(train_sub_df)}, Val: {len(val_sub_df)}, Dev: {len(dev_df)}\")\n",
    "LOG.info(f\"Unique keywords in train: {sorted(train_sub_df['keyword'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0000005",
   "metadata": {},
   "source": [
    "## 2. Keyword Vocabulary (FeatureType.KEYWORD)\n",
    "\n",
    "Build `keyword_to_idx` from `train_sub_df` only.  \n",
    "`N_KEYWORD_FEATURES = len(keyword_to_idx)` is the feature dimension registered in `EXTRA_FEAT_DICT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0000006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary from training data only (sorted for reproducibility)\n",
    "keyword_vocab    = sorted(train_sub_df[\"keyword\"].unique().tolist())\n",
    "keyword_to_idx   = {kw: i for i, kw in enumerate(keyword_vocab)}\n",
    "N_KEYWORD_FEATURES = len(keyword_vocab)\n",
    "\n",
    "# FeatureType framework: map feature type → dimension\n",
    "EXTRA_FEAT_DICT = {FeatureType.KEYWORD: N_KEYWORD_FEATURES}\n",
    "extra_feature_types = [FeatureType.KEYWORD]\n",
    "\n",
    "LOG.info(f\"FeatureType.KEYWORD — vocab size: {N_KEYWORD_FEATURES}\")\n",
    "LOG.info(f\"Keyword vocab: {keyword_vocab}\")\n",
    "\n",
    "# Sanity check: extract_keyword_feature returns correct shape\n",
    "import numpy as np\n",
    "sample = extract_keyword_feature(keyword_vocab[0], keyword_to_idx)\n",
    "assert sample.shape == (N_KEYWORD_FEATURES,) and sample.dtype == np.float32\n",
    "LOG.info(f\"extract_keyword_feature output shape: {sample.shape} ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0000007",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter Search\n",
    "\n",
    "`make_keyword_dataloaders` (from `utils/dataloaders`) builds `PCLDataset` instances\n",
    "with pre-computed `FeatureType.KEYWORD` one-hot tensors for each split,\n",
    "using `extract_keyword_feature` internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0000008",
   "metadata": {},
   "outputs": [],
   "source": [
    "POOLING_MAP = {\n",
    "    \"cls\": PoolingStrategy.CLS,\n",
    "    \"mean\": PoolingStrategy.MEAN,\n",
    "    \"max\": PoolingStrategy.MAX,\n",
    "    \"cls_mean\": PoolingStrategy.CLS_MEAN,\n",
    "}\n",
    "EXP_NAME = \"A_keyword\"\n",
    "\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    lr               = trial.suggest_float(\"lr\", 4e-6, 6e-5, log=True)\n",
    "    warmup_fraction  = trial.suggest_float(\"warmup_fraction\", 0.03, 0.20, step=0.01)\n",
    "    hidden_dim       = trial.suggest_categorical(\"hidden_dim\", [0, 128, 256, 512])\n",
    "    dropout_rate     = trial.suggest_float(\"dropout_rate\", 0.0, 0.4, step=0.05) if hidden_dim > 0 else 0.0\n",
    "    weight_decay     = trial.suggest_float(\"weight_decay\", 1e-5, 1e-2, log=True)\n",
    "    head_lr_mult     = trial.suggest_categorical(\"head_lr_multiplier\", [1, 3, 5, 10])\n",
    "    label_smoothing  = trial.suggest_float(\"label_smoothing\", 0.0, 0.15, step=0.025)\n",
    "    pooling_name     = trial.suggest_categorical(\"pooling\", [\"cls\", \"mean\", \"max\", \"cls_mean\"])\n",
    "    feat_comb_name   = trial.suggest_categorical(\"feature_comb_method\", [\"CONCAT\", \"GMF\"])\n",
    "\n",
    "    pooling   = POOLING_MAP[pooling_name]\n",
    "    feat_comb = FeatureComb.CONCAT if feat_comb_name == \"CONCAT\" else FeatureComb.GMF\n",
    "\n",
    "    LOG.info(f\"[{EXP_NAME}] Trial {trial.number}: lr={lr:.2e}, pool={pooling_name}, \"\n",
    "             f\"feat_comb={feat_comb_name}, hidden={hidden_dim}\")\n",
    "\n",
    "    # make_keyword_dataloaders uses extract_keyword_feature (FeatureType.KEYWORD)\n",
    "    train_loader, val_loader, dev_loader = make_keyword_dataloaders(\n",
    "        train_sub_df, val_sub_df, dev_df,\n",
    "        BATCH_SIZE, MAX_LENGTH, tokeniser, keyword_to_idx, DEVICE\n",
    "    )\n",
    "\n",
    "    # n_extra_features from EXTRA_FEAT_DICT, matching FeatureType.KEYWORD\n",
    "    n_extra = sum(EXTRA_FEAT_DICT[ft] for ft in extra_feature_types)\n",
    "    model = PCLDeBERTa(\n",
    "        hidden_dim=hidden_dim,\n",
    "        dropout_rate=dropout_rate,\n",
    "        n_extra_features=n_extra,\n",
    "        pooling=pooling,\n",
    "        feature_comb_method=feat_comb,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    pos_weight = compute_pos_weight(train_sub_df, DEVICE)\n",
    "\n",
    "    results = train_model(\n",
    "        model=model, device=DEVICE,\n",
    "        train_loader=train_loader, val_loader=val_loader, dev_loader=dev_loader,\n",
    "        pos_weight=pos_weight, lr=lr, weight_decay=weight_decay,\n",
    "        num_epochs=NUM_EPOCHS, warmup_fraction=warmup_fraction,\n",
    "        patience=PATIENCE, head_lr_multiplier=head_lr_mult,\n",
    "        label_smoothing=label_smoothing, eval_every_n_steps=N_EVAL_STEPS,\n",
    "        trial=trial,\n",
    "    )\n",
    "\n",
    "    trial.set_user_attr(\"best_val_f1\",    results[\"best_val_f1\"])\n",
    "    trial.set_user_attr(\"best_threshold\", results[\"best_threshold\"])\n",
    "    trial.set_user_attr(\"dev_f1\",         results[\"dev_metrics\"][\"f1\"])\n",
    "    trial.set_user_attr(\"dev_precision\",  results[\"dev_metrics\"][\"precision\"])\n",
    "    trial.set_user_attr(\"dev_recall\",     results[\"dev_metrics\"][\"recall\"])\n",
    "\n",
    "    try:\n",
    "        prev_best = trial.study.best_value\n",
    "    except ValueError:\n",
    "        prev_best = -float(\"inf\")\n",
    "    if results[\"best_val_f1\"] > prev_best:\n",
    "        torch.save(\n",
    "            {k: v.cpu() for k, v in model.state_dict().items()},\n",
    "            os.path.join(OUT_DIR, f\"exp_{EXP_NAME}_best_model.pt\")\n",
    "        )\n",
    "        config = {\n",
    "            **trial.params,\n",
    "            \"batch_size\": BATCH_SIZE, \"num_epochs\": NUM_EPOCHS, \"patience\": PATIENCE,\n",
    "            \"best_threshold\": results[\"best_threshold\"],\n",
    "            \"feature_types\": [ft.value for ft in extra_feature_types],\n",
    "            \"n_keyword_features\": N_KEYWORD_FEATURES,\n",
    "            \"keyword_vocab\": keyword_vocab,\n",
    "        }\n",
    "        with open(os.path.join(OUT_DIR, f\"exp_{EXP_NAME}_best_params.json\"), \"w\") as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        LOG.info(f\"[{EXP_NAME}] New best saved (val F1={results['best_val_f1']:.4f})\")\n",
    "\n",
    "    del model, train_loader, val_loader, dev_loader\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return results[\"best_val_f1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0000009",
   "metadata": {},
   "source": [
    "## 4. Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0000010",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=f\"pcl_deberta_exp_{EXP_NAME}\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=SEED),\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=6, n_warmup_steps=300),\n",
    ")\n",
    "study.optimize(objective, n_trials=N_TRIALS)\n",
    "\n",
    "best = study.best_trial\n",
    "LOG.info(f\"Best trial: {best.number}\")\n",
    "LOG.info(f\"Val F1: {best.user_attrs['best_val_f1']:.4f} | Dev F1: {best.user_attrs['dev_f1']:.4f}\")\n",
    "LOG.info(f\"Best params: {best.params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0000011",
   "metadata": {},
   "source": [
    "## 5. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0000012",
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot_fn, suffix in [\n",
    "    (plot_optimization_history, \"history\"),\n",
    "    (plot_param_importances, \"importances\"),\n",
    "    (plot_parallel_coordinate, \"parallel\"),\n",
    "]:\n",
    "    plot_fn(study)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUT_DIR}/{EXP_NAME}_optuna_{suffix}.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "best = study.best_trial\n",
    "best_params = best.params\n",
    "pooling   = POOLING_MAP[best_params[\"pooling\"]]\n",
    "feat_comb = FeatureComb.CONCAT if best_params[\"feature_comb_method\"] == \"CONCAT\" else FeatureComb.GMF\n",
    "n_extra   = sum(EXTRA_FEAT_DICT[ft] for ft in extra_feature_types)\n",
    "\n",
    "model = PCLDeBERTa(\n",
    "    hidden_dim=best_params[\"hidden_dim\"],\n",
    "    dropout_rate=best_params.get(\"dropout_rate\", 0.0),\n",
    "    n_extra_features=n_extra,\n",
    "    pooling=pooling,\n",
    "    feature_comb_method=feat_comb,\n",
    ").to(DEVICE)\n",
    "\n",
    "state_dict = torch.load(\n",
    "    os.path.join(OUT_DIR, f\"exp_{EXP_NAME}_best_model.pt\"), map_location=DEVICE\n",
    ")\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "_, _, dev_loader = make_keyword_dataloaders(\n",
    "    train_sub_df, val_sub_df, dev_df,\n",
    "    BATCH_SIZE, MAX_LENGTH, tokeniser, keyword_to_idx, DEVICE\n",
    ")\n",
    "dev_metrics = evaluate(model, DEVICE, dev_loader, threshold=best.user_attrs[\"best_threshold\"])\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"{EXP_NAME.upper()} — Dev Set Results (threshold={best.user_attrs['best_threshold']:.3f})\")\n",
    "print(f\"Feature types: {[ft.value for ft in extra_feature_types]} — {n_extra} dims\")\n",
    "print(f\"{'='*60}\")\n",
    "print(classification_report(dev_metrics[\"labels\"], dev_metrics[\"preds\"], target_names=[\"Non-PCL\", \"PCL\"]))\n",
    "for k, v in best_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ]
}
