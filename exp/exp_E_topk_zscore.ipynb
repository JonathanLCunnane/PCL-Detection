{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0000001",
   "metadata": {},
   "source": [
    "# Experiment E: Top-K Z-score Indicator Features\n",
    "\n",
    "Instead of 6 aggregate z-score statistics, encode the *identity* of which specific\n",
    "PCL-discriminative n-grams appear in each document as a k-dim binary indicator vector.\n",
    "\n",
    "The top-k n-grams are selected by |z-score| from the training subset only.\n",
    "k ∈ {50, 100, 200} is searched as a hyperparameter.\n",
    "Feature combination method (CONCAT / GMF) is also searched.\n",
    "\n",
    "Fixed corrections: VAL_FRACTION=0.15, BATCH_SIZE=32, NUM_EPOCHS=12, PATIENCE=4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import logging\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoTokenizer\n",
    "import spacy\n",
    "import optuna\n",
    "from optuna.visualization.matplotlib import (\n",
    "    plot_optimization_history,\n",
    "    plot_param_importances,\n",
    "    plot_parallel_coordinate,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "from utils.data import load_data\n",
    "from utils.split import split_train_val\n",
    "from utils.dataloaders import make_dataloaders\n",
    "from utils.pcl_deberta import PCLDeBERTa, PoolingStrategy\n",
    "from utils.feature_comb import FeatureComb\n",
    "from utils.fightin_words import compute_fightin_words_zscores, build_topk_ngrams, extract_topk_zscore_features\n",
    "from utils.optim import compute_pos_weight\n",
    "from utils.training_loop import train_model\n",
    "from utils.eval import evaluate\n",
    "\n",
    "SEED = 42\n",
    "DATA_DIR = \"../data\"\n",
    "OUT_DIR = \"out\"\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "MAX_LENGTH = 256\n",
    "VAL_FRACTION = 0.15\n",
    "BATCH_SIZE = 32\n",
    "N_TRIALS = 20\n",
    "NUM_EPOCHS = 12\n",
    "PATIENCE = 4\n",
    "N_EVAL_STEPS = 35\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s:\\t%(message)s\")\n",
    "LOG = logging.getLogger(__name__)\n",
    "LOG.info(f\"Device: {DEVICE}\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0000003",
   "metadata": {},
   "source": [
    "## 1. Data Loading and spaCy Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, dev_df = load_data(DATA_DIR)\n",
    "train_sub_df, val_sub_df = split_train_val(train_df, val_frac=VAL_FRACTION, seed=SEED)\n",
    "tokeniser = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "LOG.info(f\"Train: {len(train_sub_df)}, Val: {len(val_sub_df)}, Dev: {len(dev_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0000005",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "LOG.info(f\"spaCy using {'GPU' if gpu else 'CPU'}\")\n",
    "\n",
    "# Process training subset once; spaCy docs stored for feature extraction\n",
    "train_texts = train_sub_df[\"text\"].tolist()\n",
    "train_docs = list(nlp.pipe(train_texts, batch_size=256))\n",
    "LOG.info(f\"spaCy processed {len(train_docs)} train documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0000006",
   "metadata": {},
   "source": [
    "## 2. Compute Z-score Dictionary\n",
    "\n",
    "Z-score dict is computed from `train_sub_df` only (no data leakage).\n",
    "We pre-build the top-K lists for k ∈ {50, 100, 200} and scalers,\n",
    "to avoid recomputing inside the Optuna objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0000007",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_SCORES, _, _, _ = compute_fightin_words_zscores(\n",
    "    train_docs, train_sub_df[\"binary_label\"].tolist()\n",
    ")\n",
    "LOG.info(f\"Z-score dictionary: {len(Z_SCORES)} n-grams\")\n",
    "\n",
    "# Pre-build top-k ngram lists and fit scalers for k ∈ {50, 100, 200}\n",
    "K_VALUES = [50, 100, 200]\n",
    "topk_ngrams_cache: dict[int, list[str]] = {}\n",
    "scaler_cache: dict[int, StandardScaler] = {}\n",
    "\n",
    "for k in K_VALUES:\n",
    "    topk = build_topk_ngrams(Z_SCORES, k=k)\n",
    "    topk_ngrams_cache[k] = topk\n",
    "\n",
    "    # Extract train features to fit scaler\n",
    "    train_feats = np.array(\n",
    "        [extract_topk_zscore_features(doc, topk) for doc in train_docs]\n",
    "    )\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_feats)\n",
    "    scaler_cache[k] = scaler\n",
    "    LOG.info(f\"k={k}: top ngrams built, scaler fitted on {train_feats.shape}\")\n",
    "\n",
    "# Free spaCy docs after feature prep\n",
    "del train_docs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0000008",
   "metadata": {},
   "source": [
    "## 3. Feature Factory\n",
    "\n",
    "The factory runs spaCy on any text list and returns scaled k-dim indicator features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0000009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_topk_factory(k: int):\n",
    "    \"\"\"Returns an extra_feature_factory for the given k.\"\"\"\n",
    "    topk = topk_ngrams_cache[k]\n",
    "    scaler = scaler_cache[k]\n",
    "\n",
    "    def factory(texts: list[str]) -> torch.Tensor:\n",
    "        feats = np.array(\n",
    "            [extract_topk_zscore_features(doc, topk) for doc in nlp.pipe(texts, batch_size=256)]\n",
    "        )\n",
    "        scaled = scaler.transform(feats).astype(np.float32)\n",
    "        return torch.tensor(scaled).to(DEVICE)\n",
    "\n",
    "    return factory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0000010",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0000011",
   "metadata": {},
   "outputs": [],
   "source": [
    "POOLING_MAP = {\n",
    "    \"cls\": PoolingStrategy.CLS,\n",
    "    \"mean\": PoolingStrategy.MEAN,\n",
    "    \"max\": PoolingStrategy.MAX,\n",
    "    \"cls_mean\": PoolingStrategy.CLS_MEAN,\n",
    "}\n",
    "EXP_NAME = \"E_topk_zscore\"\n",
    "\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    lr              = trial.suggest_float(\"lr\", 4e-6, 6e-5, log=True)\n",
    "    warmup_fraction = trial.suggest_float(\"warmup_fraction\", 0.03, 0.20, step=0.01)\n",
    "    hidden_dim      = trial.suggest_categorical(\"hidden_dim\", [0, 128, 256, 512])\n",
    "    dropout_rate    = trial.suggest_float(\"dropout_rate\", 0.0, 0.4, step=0.05) if hidden_dim > 0 else 0.0\n",
    "    weight_decay    = trial.suggest_float(\"weight_decay\", 1e-5, 1e-2, log=True)\n",
    "    head_lr_mult    = trial.suggest_categorical(\"head_lr_multiplier\", [1, 3, 5, 10])\n",
    "    label_smoothing = trial.suggest_float(\"label_smoothing\", 0.0, 0.15, step=0.025)\n",
    "    pooling_name    = trial.suggest_categorical(\"pooling\", [\"cls\", \"mean\", \"max\", \"cls_mean\"])\n",
    "    k               = trial.suggest_categorical(\"k\", [50, 100, 200])\n",
    "    feat_comb_name  = trial.suggest_categorical(\"feature_comb_method\", [\"CONCAT\", \"GMF\"])\n",
    "\n",
    "    pooling   = POOLING_MAP[pooling_name]\n",
    "    feat_comb = FeatureComb.CONCAT if feat_comb_name == \"CONCAT\" else FeatureComb.GMF\n",
    "\n",
    "    LOG.info(f\"[{EXP_NAME}] Trial {trial.number}: lr={lr:.2e}, k={k}, \"\n",
    "             f\"feat_comb={feat_comb_name}, pool={pooling_name}\")\n",
    "\n",
    "    factory = make_topk_factory(k)\n",
    "    train_loader, val_loader, dev_loader = make_dataloaders(\n",
    "        train_sub_df, val_sub_df, dev_df, BATCH_SIZE, MAX_LENGTH, tokeniser, factory\n",
    "    )\n",
    "\n",
    "    model = PCLDeBERTa(\n",
    "        hidden_dim=hidden_dim,\n",
    "        dropout_rate=dropout_rate,\n",
    "        n_extra_features=k,\n",
    "        pooling=pooling,\n",
    "        feature_comb_method=feat_comb,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    pos_weight = compute_pos_weight(train_sub_df, DEVICE)\n",
    "\n",
    "    results = train_model(\n",
    "        model=model, device=DEVICE,\n",
    "        train_loader=train_loader, val_loader=val_loader, dev_loader=dev_loader,\n",
    "        pos_weight=pos_weight, lr=lr, weight_decay=weight_decay,\n",
    "        num_epochs=NUM_EPOCHS, warmup_fraction=warmup_fraction,\n",
    "        patience=PATIENCE, head_lr_multiplier=head_lr_mult,\n",
    "        label_smoothing=label_smoothing, eval_every_n_steps=N_EVAL_STEPS,\n",
    "        trial=trial,\n",
    "    )\n",
    "\n",
    "    trial.set_user_attr(\"best_val_f1\",    results[\"best_val_f1\"])\n",
    "    trial.set_user_attr(\"best_threshold\", results[\"best_threshold\"])\n",
    "    trial.set_user_attr(\"dev_f1\",         results[\"dev_metrics\"][\"f1\"])\n",
    "    trial.set_user_attr(\"dev_precision\",  results[\"dev_metrics\"][\"precision\"])\n",
    "    trial.set_user_attr(\"dev_recall\",     results[\"dev_metrics\"][\"recall\"])\n",
    "\n",
    "    try:\n",
    "        prev_best = trial.study.best_value\n",
    "    except ValueError:\n",
    "        prev_best = -float(\"inf\")\n",
    "    if results[\"best_val_f1\"] > prev_best:\n",
    "        torch.save(\n",
    "            {k: v.cpu() for k, v in model.state_dict().items()},\n",
    "            os.path.join(OUT_DIR, f\"exp_{EXP_NAME}_best_model.pt\")\n",
    "        )\n",
    "        config = {**trial.params, \"batch_size\": BATCH_SIZE, \"num_epochs\": NUM_EPOCHS,\n",
    "                  \"patience\": PATIENCE, \"best_threshold\": results[\"best_threshold\"]}\n",
    "        with open(os.path.join(OUT_DIR, f\"exp_{EXP_NAME}_best_params.json\"), \"w\") as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        LOG.info(f\"[{EXP_NAME}] New best saved (val F1={results['best_val_f1']:.4f})\")\n",
    "\n",
    "    del model, train_loader, val_loader, dev_loader\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return results[\"best_val_f1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0000012",
   "metadata": {},
   "source": [
    "## 5. Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0000013",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=f\"pcl_deberta_exp_{EXP_NAME}\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=SEED),\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=6, n_warmup_steps=300),\n",
    ")\n",
    "study.optimize(objective, n_trials=N_TRIALS)\n",
    "\n",
    "best = study.best_trial\n",
    "LOG.info(f\"Best trial: {best.number}\")\n",
    "LOG.info(f\"Val F1: {best.user_attrs['best_val_f1']:.4f} | Dev F1: {best.user_attrs['dev_f1']:.4f}\")\n",
    "LOG.info(f\"Best params: {best.params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0000014",
   "metadata": {},
   "source": [
    "## 6. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0000015",
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot_fn, suffix in [\n",
    "    (plot_optimization_history, \"history\"),\n",
    "    (plot_param_importances, \"importances\"),\n",
    "    (plot_parallel_coordinate, \"parallel\"),\n",
    "]:\n",
    "    plot_fn(study)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUT_DIR}/{EXP_NAME}_optuna_{suffix}.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "best = study.best_trial\n",
    "best_params = best.params\n",
    "pooling = POOLING_MAP[best_params[\"pooling\"]]\n",
    "feat_comb = FeatureComb.CONCAT if best_params[\"feature_comb_method\"] == \"CONCAT\" else FeatureComb.GMF\n",
    "k_best = best_params[\"k\"]\n",
    "\n",
    "model = PCLDeBERTa(\n",
    "    hidden_dim=best_params[\"hidden_dim\"],\n",
    "    dropout_rate=best_params.get(\"dropout_rate\", 0.0),\n",
    "    n_extra_features=k_best,\n",
    "    pooling=pooling,\n",
    "    feature_comb_method=feat_comb,\n",
    ").to(DEVICE)\n",
    "\n",
    "state_dict = torch.load(\n",
    "    os.path.join(OUT_DIR, f\"exp_{EXP_NAME}_best_model.pt\"), map_location=DEVICE\n",
    ")\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "factory = make_topk_factory(k_best)\n",
    "_, _, dev_loader = make_dataloaders(\n",
    "    train_sub_df, val_sub_df, dev_df, BATCH_SIZE, MAX_LENGTH, tokeniser, factory\n",
    ")\n",
    "dev_metrics = evaluate(model, DEVICE, dev_loader, threshold=best.user_attrs[\"best_threshold\"])\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"{EXP_NAME.upper()} — Dev Set Results (threshold={best.user_attrs['best_threshold']:.3f})\")\n",
    "print(f\"{'='*60}\")\n",
    "print(classification_report(dev_metrics[\"labels\"], dev_metrics[\"preds\"], target_names=[\"Non-PCL\", \"PCL\"]))\n",
    "for param_k, param_v in best_params.items():\n",
    "    print(f\"  {param_k}: {param_v}\")\n",
    "\n",
    "# Show most discriminative n-grams used as features\n",
    "print(f\"\\nTop-10 PCL n-grams (highest +z):\")\n",
    "topk = topk_ngrams_cache[k_best]\n",
    "for ng in topk[:10]:\n",
    "    print(f\"  {ng!r:30s}  z={Z_SCORES[ng]:+.2f}\")\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ]
}
