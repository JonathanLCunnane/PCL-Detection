{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "g0000001",
   "metadata": {},
   "source": [
    "# Experiment G: Weighted Random Sampling\n",
    "\n",
    "Address the ~10:1 class imbalance via `WeightedRandomSampler` on the training\n",
    "DataLoader rather than via `pos_weight` in the loss.\n",
    "\n",
    "**Key design choices:**\n",
    "- `target_pos_frac` (searched) controls the expected fraction of PCL samples\n",
    "  per batch — 0.5 = balanced, 0.095 ≈ natural distribution.\n",
    "- `pos_weight = 1.0` in BCE loss — sampler already handles imbalance; using\n",
    "  both would double-compensate and over-emphasise the minority class.\n",
    "- Sampler uses `replacement=True` with `num_samples=len(train_sub_df)` so\n",
    "  each epoch sees approximately the same number of gradient steps as before.\n",
    "- `train_model` from `utils/training_loop.py` is used **unchanged**.\n",
    "- Val and dev loaders are unweighted (evaluate on true distribution).\n",
    "\n",
    "Fixed hyperparameters (not searched):\n",
    "- VAL_FRACTION=0.15, BATCH_SIZE=32, NUM_EPOCHS=12, PATIENCE=4\n",
    "- `pooling=MEAN` — best default for DeBERTa-v3 (RTD pretraining; no special CLS)\n",
    "- `warmup_fraction=0.10`, `label_smoothing=0.0` — fixed to save trials for key params\n",
    "\n",
    "Searched: `lr`, `weight_decay`, `hidden_dim ∈ {0, 256}`, `dropout_rate`,\n",
    "`head_lr_multiplier ∈ {1, 3, 5}`, `target_pos_frac ∈ {0.2, 0.35, 0.5}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g0000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import logging\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.metrics import classification_report\n",
    "import optuna\n",
    "from optuna.visualization.matplotlib import (\n",
    "    plot_optimization_history,\n",
    "    plot_param_importances,\n",
    "    plot_parallel_coordinate,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "from utils.data import load_data\n",
    "from utils.split import split_train_val\n",
    "from utils.dataloaders import make_weighted_dataloaders\n",
    "from utils.pcl_deberta import PCLDeBERTa, PoolingStrategy\n",
    "from utils.training_loop import train_model\n",
    "from utils.eval import evaluate\n",
    "\n",
    "SEED = 42\n",
    "DATA_DIR = \"../data\"\n",
    "OUT_DIR = \"out\"\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "MAX_LENGTH = 256\n",
    "VAL_FRACTION = 0.15\n",
    "BATCH_SIZE = 32\n",
    "N_TRIALS = 20\n",
    "NUM_EPOCHS = 12\n",
    "PATIENCE = 4\n",
    "N_EVAL_STEPS = 35\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s:\\t%(message)s\")\n",
    "LOG = logging.getLogger(__name__)\n",
    "LOG.info(f\"Device: {DEVICE}\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g0000003",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g0000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, dev_df = load_data(DATA_DIR)\n",
    "train_sub_df, val_sub_df = split_train_val(train_df, val_frac=VAL_FRACTION, seed=SEED)\n",
    "tokeniser = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "n_pos = int(train_sub_df[\"binary_label\"].sum())\n",
    "n_neg = len(train_sub_df) - n_pos\n",
    "LOG.info(f\"Train: {len(train_sub_df)} ({n_pos} PCL / {n_neg} non-PCL, \"\n",
    "         f\"natural pos frac={n_pos/len(train_sub_df):.3f})\")\n",
    "LOG.info(f\"Val: {len(val_sub_df)}, Dev: {len(dev_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g0000005",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter Search\n",
    "\n",
    "`target_pos_frac` is the key new hyperparameter. We search three values:\n",
    "- `0.5` — fully balanced batches\n",
    "- `0.35` — moderately oversampled\n",
    "- `0.2` — mild oversampling (roughly 2× natural frequency)\n",
    "\n",
    "Secondary hyperparameters are narrowed to avoid wasting trials:\n",
    "- `pooling` fixed to MEAN (DeBERTa-v3 RTD pretraining; no special CLS token)\n",
    "- `warmup_fraction` fixed to 0.10, `label_smoothing` fixed to 0.0\n",
    "- `hidden_dim ∈ {0, 256}` (0 = single linear, 256 = MLP)\n",
    "- `dropout_rate ∈ {0.1, 0.3}` (only sampled when hidden_dim=256)\n",
    "- `head_lr_multiplier ∈ {1, 3, 5}`\n",
    "\n",
    "`pos_weight=1.0` is passed to `train_model` — the sampler handles class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g0000006",
   "metadata": {},
   "outputs": [],
   "source": [
    "POOLING = PoolingStrategy.MEAN   # fixed: best default for DeBERTa-v3 RTD pretraining\n",
    "EXP_NAME = \"G_weighted_sampling\"\n",
    "\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    lr              = trial.suggest_float(\"lr\", 4e-6, 6e-5, log=True)\n",
    "    weight_decay    = trial.suggest_float(\"weight_decay\", 1e-5, 1e-2, log=True)\n",
    "    hidden_dim      = trial.suggest_categorical(\"hidden_dim\", [0, 256])\n",
    "    dropout_rate    = trial.suggest_categorical(\"dropout_rate\", [0.1, 0.3]) if hidden_dim > 0 else 0.0\n",
    "    head_lr_mult    = trial.suggest_categorical(\"head_lr_multiplier\", [1, 3, 5, 10])\n",
    "    target_pos_frac = trial.suggest_categorical(\"target_pos_frac\", [0.2, 0.35, 0.5])\n",
    "\n",
    "    # Fixed — not worth spending trials on\n",
    "    warmup_fraction = 0.10\n",
    "    label_smoothing = 0.0\n",
    "\n",
    "    LOG.info(f\"[{EXP_NAME}] Trial {trial.number}: lr={lr:.2e}, hidden={hidden_dim}, \"\n",
    "             f\"target_pos_frac={target_pos_frac}, head_lr_mult={head_lr_mult}\")\n",
    "\n",
    "    train_loader, val_loader, dev_loader = make_weighted_dataloaders(\n",
    "        train_sub_df, val_sub_df, dev_df, BATCH_SIZE, MAX_LENGTH, tokeniser,\n",
    "        target_pos_frac=target_pos_frac,\n",
    "    )\n",
    "\n",
    "    model = PCLDeBERTa(\n",
    "        hidden_dim=hidden_dim, dropout_rate=dropout_rate, pooling=POOLING\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # pos_weight=1.0 — sampler handles imbalance; no double-compensation\n",
    "    pos_weight = torch.ones(1, device=DEVICE)\n",
    "\n",
    "    results = train_model(\n",
    "        model=model, device=DEVICE,\n",
    "        train_loader=train_loader, val_loader=val_loader, dev_loader=dev_loader,\n",
    "        pos_weight=pos_weight, lr=lr, weight_decay=weight_decay,\n",
    "        num_epochs=NUM_EPOCHS, warmup_fraction=warmup_fraction,\n",
    "        patience=PATIENCE,\n",
    "        head_lr_multiplier=head_lr_mult,\n",
    "        label_smoothing=label_smoothing,\n",
    "        eval_every_n_steps=N_EVAL_STEPS,\n",
    "        trial=trial,\n",
    "    )\n",
    "\n",
    "    trial.set_user_attr(\"best_val_f1\",    results[\"best_val_f1\"])\n",
    "    trial.set_user_attr(\"best_threshold\", results[\"best_threshold\"])\n",
    "    trial.set_user_attr(\"dev_f1\",         results[\"dev_metrics\"][\"f1\"])\n",
    "    trial.set_user_attr(\"dev_precision\",  results[\"dev_metrics\"][\"precision\"])\n",
    "    trial.set_user_attr(\"dev_recall\",     results[\"dev_metrics\"][\"recall\"])\n",
    "\n",
    "    try:\n",
    "        prev_best = trial.study.best_value\n",
    "    except ValueError:\n",
    "        prev_best = -float(\"inf\")\n",
    "    if results[\"best_val_f1\"] > prev_best:\n",
    "        torch.save(\n",
    "            {k: v.cpu() for k, v in model.state_dict().items()},\n",
    "            os.path.join(OUT_DIR, f\"exp_{EXP_NAME}_best_model.pt\")\n",
    "        )\n",
    "        config = {\n",
    "            **trial.params,\n",
    "            \"pooling\": \"mean\",\n",
    "            \"warmup_fraction\": warmup_fraction,\n",
    "            \"label_smoothing\": label_smoothing,\n",
    "            \"batch_size\": BATCH_SIZE, \"num_epochs\": NUM_EPOCHS, \"patience\": PATIENCE,\n",
    "            \"best_threshold\": results[\"best_threshold\"],\n",
    "        }\n",
    "        with open(os.path.join(OUT_DIR, f\"exp_{EXP_NAME}_best_params.json\"), \"w\") as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        LOG.info(f\"[{EXP_NAME}] New best saved (val F1={results['best_val_f1']:.4f})\")\n",
    "\n",
    "    del model, train_loader, val_loader, dev_loader\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return results[\"best_val_f1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g0000007",
   "metadata": {},
   "source": [
    "## 3. Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g0000008",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=f\"pcl_deberta_exp_{EXP_NAME}\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=SEED),\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=6, n_warmup_steps=300),\n",
    ")\n",
    "study.optimize(objective, n_trials=N_TRIALS)\n",
    "\n",
    "best = study.best_trial\n",
    "LOG.info(f\"Best trial: {best.number}\")\n",
    "LOG.info(f\"Val F1: {best.user_attrs['best_val_f1']:.4f} | Dev F1: {best.user_attrs['dev_f1']:.4f}\")\n",
    "LOG.info(f\"Best params: {best.params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g0000009",
   "metadata": {},
   "source": [
    "## 4. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g0000010",
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot_fn, suffix in [\n",
    "    (plot_optimization_history, \"history\"),\n",
    "    (plot_param_importances, \"importances\"),\n",
    "    (plot_parallel_coordinate, \"parallel\"),\n",
    "]:\n",
    "    plot_fn(study)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUT_DIR}/{EXP_NAME}_optuna_{suffix}.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "best = study.best_trial\n",
    "best_params = best.params\n",
    "\n",
    "model = PCLDeBERTa(\n",
    "    hidden_dim=best_params[\"hidden_dim\"],\n",
    "    dropout_rate=best_params.get(\"dropout_rate\", 0.0),\n",
    "    pooling=POOLING,\n",
    ").to(DEVICE)\n",
    "\n",
    "state_dict = torch.load(\n",
    "    os.path.join(OUT_DIR, f\"exp_{EXP_NAME}_best_model.pt\"), map_location=DEVICE\n",
    ")\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "_, _, dev_loader = make_weighted_dataloaders(\n",
    "    train_sub_df, val_sub_df, dev_df, BATCH_SIZE, MAX_LENGTH, tokeniser,\n",
    "    target_pos_frac=best_params[\"target_pos_frac\"],\n",
    ")\n",
    "\n",
    "best_threshold = best.user_attrs[\"best_threshold\"]\n",
    "dev_metrics = evaluate(model, DEVICE, dev_loader, threshold=best_threshold)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"{EXP_NAME.upper()} — Dev Set Results (threshold={best_threshold:.3f})\")\n",
    "print(f\"{'='*60}\")\n",
    "print(classification_report(dev_metrics[\"labels\"], dev_metrics[\"preds\"],\n",
    "                             target_names=[\"Non-PCL\", \"PCL\"]))\n",
    "print(\"Best hyperparams:\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(f\"  pooling: mean (fixed)\")\n",
    "print(f\"  warmup_fraction: 0.10 (fixed)\")\n",
    "print(f\"  label_smoothing: 0.0 (fixed)\")\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
