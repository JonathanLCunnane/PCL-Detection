{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0000001",
   "metadata": {},
   "source": [
    "# Experiment F: DeBERTa-v3-large\n",
    "\n",
    "Fine-tune `microsoft/deberta-v3-large` (~400M params, hidden=1024, 24 layers) on the PCL task.\n",
    "\n",
    "## Memory strategy\n",
    "\n",
    "DeBERTa-v3 is **incompatible with gradient checkpointing + gradient accumulation** due to its\n",
    "relative position embedding cache being part of the computational graph. With gradient accumulation,\n",
    "micro-batch 2's backward tries to reuse cached position tensors already freed by micro-batch 1:\n",
    "- `use_reentrant=False` → tensor count mismatch error\n",
    "- `use_reentrant=True` → \"backward through freed graph\" error\n",
    "\n",
    "**Solution:** disable gradient checkpointing and rely on **bitsandbytes 8-bit AdamW** alone.\n",
    "\n",
    "| Component | Memory |\n",
    "|---|---|\n",
    "| Weights (fp32) | 1.68 GB |\n",
    "| Gradients (fp32) | 1.68 GB |\n",
    "| 8-bit optimizer states | ~0.84 GB |\n",
    "| Activations (no checkpointing, bs=4, seq=256) | ~1.0 GB |\n",
    "| CUDA overhead | ~0.3 GB |\n",
    "| **Total** | **~5.5 GB** ✓ |\n",
    "\n",
    "`MICRO_BATCH_SIZE=4`, `ACCUMULATION_STEPS=8` → effective batch=32. Much better GPU utilisation\n",
    "than batch=1, and memory is comfortably within 8 GB.\n",
    "\n",
    "**No extra features** — isolates the effect of the larger backbone.\n",
    "\n",
    "**Fixed corrections:** VAL_FRACTION=0.15, effective BATCH_SIZE=32, NUM_EPOCHS=12, PATIENCE=4,\n",
    "pooling searched over CLS/MEAN/MAX/CLS_MEAN (SCALAR_MIX excluded — hardcodes 13 hidden states)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0000002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-28 23:15:32,788 INFO:\tDevice: cuda\n",
      "2026-02-28 23:15:32,789 INFO:\tbitsandbytes 0.49.2 available — will use 8-bit AdamW\n",
      "2026-02-28 23:15:32,790 INFO:\tGPU: NVIDIA GeForce RTX 3060 Ti | VRAM: 8.0 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import logging\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.metrics import classification_report\n",
    "import optuna\n",
    "from optuna.visualization.matplotlib import (\n",
    "    plot_optimization_history,\n",
    "    plot_param_importances,\n",
    "    plot_parallel_coordinate,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "from utils.data import load_data\n",
    "from utils.split import split_train_val\n",
    "from utils.dataloaders import make_dataloaders\n",
    "from utils.pcl_deberta import PCLDeBERTa, PoolingStrategy\n",
    "from utils.optim import compute_pos_weight\n",
    "from utils.training_loop import train_model\n",
    "from utils.eval import evaluate\n",
    "\n",
    "SEED = 42\n",
    "DATA_DIR = \"../data\"\n",
    "OUT_DIR = \"out\"\n",
    "MODEL_NAME = \"microsoft/deberta-v3-large\"\n",
    "MAX_LENGTH = 256\n",
    "VAL_FRACTION = 0.15\n",
    "# Gradient checkpointing is disabled (incompatible with DeBERTa-v3 + grad accumulation).\n",
    "# Rely on bitsandbytes 8-bit Adam for memory savings instead.\n",
    "MICRO_BATCH_SIZE = 2       # no checkpointing: bs=4 uses ~1 GB activations → ~5.5 GB total\n",
    "ACCUMULATION_STEPS = 16    # effective batch size = 4 * 8 = 32\n",
    "N_TRIALS = 10\n",
    "NUM_EPOCHS = 12\n",
    "PATIENCE = 4\n",
    "N_EVAL_STEPS = 35          # in optimizer steps (not micro-steps)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s:\\t%(message)s\")\n",
    "LOG = logging.getLogger(__name__)\n",
    "LOG.info(f\"Device: {DEVICE}\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Check bitsandbytes availability\n",
    "try:\n",
    "    import bitsandbytes as bnb\n",
    "    USE_8BIT_ADAM = True\n",
    "    LOG.info(f\"bitsandbytes {bnb.__version__} available — will use 8-bit AdamW\")\n",
    "except ImportError:\n",
    "    USE_8BIT_ADAM = False\n",
    "    LOG.warning(\"bitsandbytes not installed — using standard AdamW (may OOM on 8 GB)\")\n",
    "    LOG.warning(\"Install with: pip install bitsandbytes>=0.43\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    total_vram = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    LOG.info(f\"GPU: {torch.cuda.get_device_name(0)} | VRAM: {total_vram:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0000004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-28 23:15:33,008 INFO:\tTrain/val split: 7118 train, 1257 val (val_frac=0.15)\n",
      "2026-02-28 23:15:33,009 INFO:\tTrain, val positive count: 675, 119\n",
      "2026-02-28 23:15:33,136 INFO:\tHTTP Request: HEAD https://huggingface.co/microsoft/deberta-v3-large/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-28 23:15:33,145 INFO:\tHTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/microsoft/deberta-v3-large/64a8c8eab3e352a784c658aef62be1662607476f/config.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-28 23:15:33,247 INFO:\tHTTP Request: HEAD https://huggingface.co/microsoft/deberta-v3-large/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-28 23:15:33,255 INFO:\tHTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/microsoft/deberta-v3-large/64a8c8eab3e352a784c658aef62be1662607476f/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-28 23:15:33,356 INFO:\tHTTP Request: GET https://huggingface.co/api/models/microsoft/deberta-v3-large/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
      "2026-02-28 23:15:33,457 INFO:\tHTTP Request: GET https://huggingface.co/api/models/microsoft/deberta-v3-large/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
      "2026-02-28 23:15:33,925 INFO:\tHTTP Request: GET https://huggingface.co/api/models/microsoft/deberta-v3-large \"HTTP/1.1 200 OK\"\n",
      "2026-02-28 23:15:33,927 INFO:\tTrain: 7118, Val: 1257, Dev: 2093\n"
     ]
    }
   ],
   "source": [
    "train_df, dev_df = load_data(DATA_DIR)\n",
    "train_sub_df, val_sub_df = split_train_val(train_df, val_frac=VAL_FRACTION, seed=SEED)\n",
    "tokeniser = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "LOG.info(f\"Train: {len(train_sub_df)}, Val: {len(val_sub_df)}, Dev: {len(dev_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0000005",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter Search\n",
    "\n",
    "With only 10 trials, the full search space (~54,000 discrete combinations) is untractable —\n",
    "TPE degenerates to near-random sampling. We narrow to ~72 discrete combinations by:\n",
    "\n",
    "| Parameter | Change | Reason |\n",
    "|---|---|---|\n",
    "| `warmup_fraction` | **Fixed at 0.10** | Standard reliable value; 18 options wasted |\n",
    "| `label_smoothing` | **Fixed at 0.0** | Rarely decisive at this scale |\n",
    "| `hidden_dim` | `[0, 256]` (drop 128) | 256 is the meaningful MLP choice |\n",
    "| `dropout_rate` | `[0.1, 0.3]` step=0.1 | Tighter range; ≤0.05 too close to 0 |\n",
    "| `head_lr_mult` | `[1, 3, 5]` (drop 10) | 10× too aggressive for large models |\n",
    "| `lr` | `[2e-6, 2e-5]` (narrowed) | Large models prefer lower LR than base |\n",
    "| `pooling` | Keep all 4 | Strong structural impact; unknown for large |\n",
    "| `weight_decay` | `[1e-4, 5e-3]` (narrowed) | Tighter, well-motivated range |\n",
    "\n",
    "Resulting discrete combinations: `2 (hidden) × 3 (dropout) × 3 (head_lr) × 4 (pooling)` = **72**,\n",
    "plus two continuous dims (`lr`, `weight_decay`). 10 TPE trials can meaningfully explore this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0000006",
   "metadata": {},
   "outputs": [],
   "source": [
    "POOLING_MAP = {\n",
    "    \"cls\": PoolingStrategy.CLS,\n",
    "    \"mean\": PoolingStrategy.MEAN,\n",
    "    \"max\": PoolingStrategy.MAX,\n",
    "    \"cls_mean\": PoolingStrategy.CLS_MEAN,\n",
    "}\n",
    "EXP_NAME = \"F_large\"\n",
    "\n",
    "WARMUP_FRACTION = 0.10   # fixed — reliable standard value\n",
    "LABEL_SMOOTHING = 0.0    # fixed — rarely decisive for this task\n",
    "POOLING = PoolingStrategy.CLS_MEAN  # fixed — generally strong\n",
    "\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    lr            = trial.suggest_float(\"lr\", 2e-6, 2e-5, log=True)\n",
    "    hidden_dim    = trial.suggest_categorical(\"hidden_dim\", [0, 256])\n",
    "    dropout_rate  = trial.suggest_float(\"dropout_rate\", 0.1, 0.3, step=0.1) if hidden_dim > 0 else 0.0\n",
    "    weight_decay  = trial.suggest_float(\"weight_decay\", 1e-4, 5e-3, log=True)\n",
    "    head_lr_mult  = trial.suggest_categorical(\"head_lr_multiplier\", [3, 5, 10])\n",
    "\n",
    "    LOG.info(f\"[{EXP_NAME}] Trial {trial.number}: lr={lr:.2e}, pool={POOLING}, \"\n",
    "             f\"hidden={hidden_dim}, wd={weight_decay:.1e}, head_lr_mult={head_lr_mult}\")\n",
    "\n",
    "    train_loader, val_loader, dev_loader = make_dataloaders(\n",
    "        train_sub_df, val_sub_df, dev_df, MICRO_BATCH_SIZE, MAX_LENGTH, tokeniser\n",
    "    )\n",
    "\n",
    "    model = PCLDeBERTa(\n",
    "        hidden_dim=hidden_dim,\n",
    "        dropout_rate=dropout_rate,\n",
    "        pooling=POOLING,\n",
    "        model_name=MODEL_NAME,\n",
    "        gradient_checkpointing=False,  # incompatible with DeBERTa-v3 + grad accumulation\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    pos_weight = compute_pos_weight(train_sub_df, DEVICE)\n",
    "\n",
    "    results = train_model(\n",
    "        model=model, device=DEVICE,\n",
    "        train_loader=train_loader, val_loader=val_loader, dev_loader=dev_loader,\n",
    "        pos_weight=pos_weight, lr=lr, weight_decay=weight_decay,\n",
    "        num_epochs=NUM_EPOCHS, warmup_fraction=WARMUP_FRACTION,\n",
    "        patience=PATIENCE, head_lr_multiplier=head_lr_mult,\n",
    "        label_smoothing=LABEL_SMOOTHING,\n",
    "        eval_every_n_steps=N_EVAL_STEPS,\n",
    "        accumulate_grad_batches=ACCUMULATION_STEPS,\n",
    "        use_8bit_adam=USE_8BIT_ADAM,\n",
    "        trial=trial,\n",
    "    )\n",
    "\n",
    "    trial.set_user_attr(\"best_val_f1\",    results[\"best_val_f1\"])\n",
    "    trial.set_user_attr(\"best_threshold\", results[\"best_threshold\"])\n",
    "    trial.set_user_attr(\"dev_f1\",         results[\"dev_metrics\"][\"f1\"])\n",
    "    trial.set_user_attr(\"dev_precision\",  results[\"dev_metrics\"][\"precision\"])\n",
    "    trial.set_user_attr(\"dev_recall\",     results[\"dev_metrics\"][\"recall\"])\n",
    "\n",
    "    try:\n",
    "        prev_best = trial.study.best_value\n",
    "    except ValueError:\n",
    "        prev_best = -float(\"inf\")\n",
    "    if results[\"best_val_f1\"] > prev_best:\n",
    "        torch.save(\n",
    "            {k: v.cpu() for k, v in model.state_dict().items()},\n",
    "            os.path.join(OUT_DIR, f\"exp_{EXP_NAME}_best_model.pt\")\n",
    "        )\n",
    "        config = {\n",
    "            **trial.params,\n",
    "            \"warmup_fraction\": WARMUP_FRACTION,\n",
    "            \"label_smoothing\": LABEL_SMOOTHING,\n",
    "            \"pooling\": POOLING.name,\n",
    "            \"batch_size\": MICRO_BATCH_SIZE,\n",
    "            \"accumulation_steps\": ACCUMULATION_STEPS,\n",
    "            \"effective_batch_size\": MICRO_BATCH_SIZE * ACCUMULATION_STEPS,\n",
    "            \"num_epochs\": NUM_EPOCHS,\n",
    "            \"patience\": PATIENCE,\n",
    "            \"best_threshold\": results[\"best_threshold\"],\n",
    "            \"model_name\": MODEL_NAME,\n",
    "        }\n",
    "        with open(os.path.join(OUT_DIR, f\"exp_{EXP_NAME}_best_params.json\"), \"w\") as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        LOG.info(f\"[{EXP_NAME}] New best saved (val F1={results['best_val_f1']:.4f})\")\n",
    "\n",
    "    del model, train_loader, val_loader, dev_loader\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return results[\"best_val_f1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0000007",
   "metadata": {},
   "source": [
    "## 3. Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a16e97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0000008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-28 23:15:34,742]\u001b[0m A new study created in memory with name: pcl_deberta_exp_F_large\u001b[0m\n",
      "2026-02-28 23:15:34,745 INFO:\t[F_large] Trial 0: lr=4.74e-06, pool=PoolingStrategy.CLS_MEAN, hidden=0, wd=1.0e-03, head_lr_mult=3\n",
      "2026-02-28 23:15:38,434 INFO:\tHTTP Request: HEAD https://huggingface.co/microsoft/deberta-v3-large/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-02-28 23:15:38,443 INFO:\tHTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/microsoft/deberta-v3-large/64a8c8eab3e352a784c658aef62be1662607476f/config.json \"HTTP/1.1 200 OK\"\n",
      "2026-02-28 23:15:38,743 INFO:\tHTTP Request: HEAD https://huggingface.co/microsoft/deberta-v3-large/resolve/main/model.safetensors \"HTTP/1.1 404 Not Found\"\n",
      "2026-02-28 23:15:38,845 INFO:\tHTTP Request: GET https://huggingface.co/api/models/microsoft/deberta-v3-large \"HTTP/1.1 200 OK\"\n",
      "2026-02-28 23:15:38,965 INFO:\tHTTP Request: GET https://huggingface.co/api/models/microsoft/deberta-v3-large/commits/main \"HTTP/1.1 200 OK\"\n",
      "2026-02-28 23:15:39,074 INFO:\tHTTP Request: GET https://huggingface.co/api/models/microsoft/deberta-v3-large/discussions?p=0 \"HTTP/1.1 200 OK\"\n",
      "2026-02-28 23:15:39,190 INFO:\tHTTP Request: GET https://huggingface.co/api/models/microsoft/deberta-v3-large/commits/refs%2Fpr%2F13 \"HTTP/1.1 200 OK\"\n",
      "2026-02-28 23:15:39,288 INFO:\tHTTP Request: HEAD https://huggingface.co/microsoft/deberta-v3-large/resolve/refs%2Fpr%2F13/model.safetensors.index.json \"HTTP/1.1 404 Not Found\"\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "2026-02-28 23:15:39,289 WARNING:\tWarning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "2026-02-28 23:15:39,399 INFO:\tHTTP Request: HEAD https://huggingface.co/microsoft/deberta-v3-large/resolve/refs%2Fpr%2F13/model.safetensors \"HTTP/1.1 302 Found\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6da2790d06e466986c897ce97adb440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mDebertaV2Model LOAD REPORT\u001b[0m from: microsoft/deberta-v3-large\n",
      "Key                                     | Status     |  | \n",
      "----------------------------------------+------------+--+-\n",
      "mask_predictions.dense.weight           | UNEXPECTED |  | \n",
      "mask_predictions.dense.bias             | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED |  | \n",
      "mask_predictions.classifier.weight      | UNEXPECTED |  | \n",
      "mask_predictions.LayerNorm.weight       | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED |  | \n",
      "mask_predictions.LayerNorm.bias         | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED |  | \n",
      "mask_predictions.classifier.bias        | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "2026-02-28 23:15:41,233 INFO:\tBackbone model loaded: DebertaV2Model, dtype torch.float32, gradient_checkpointing=False\n",
      "2026-02-28 23:15:42,015 INFO:\tpos_weight = 3.09 (raw ratio=9.55, neg=6443, pos=675)\n",
      "2026-02-28 23:15:42,019 INFO:\tUsing bitsandbytes 8-bit AdamW\n",
      "2026-02-28 23:15:42,020 INFO:\tEarly stopping: patience=4 epochs = 24 eval rounds (6 evals/epoch, eval every 35 steps)\n",
      "2026-02-28 23:15:42,021 INFO:\tGradient accumulation: 16 micro-steps per optimizer step\n",
      "2026-02-28 23:15:42,024 INFO:\tEpoch 1/12\n",
      "2026-02-28 23:18:53,104 INFO:\tStep 35 | Train Loss: 0.7351 | Val Loss: 0.7087 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-02-28 23:21:58,860 INFO:\tStep 70 | Train Loss: 0.6474 | Val Loss: 0.6753 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-02-28 23:25:04,425 INFO:\tStep 105 | Train Loss: 0.6560 | Val Loss: 0.6670 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-02-28 23:28:08,817 INFO:\tStep 140 | Train Loss: 0.7083 | Val Loss: 0.6650 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-02-28 23:31:13,263 INFO:\tStep 175 | Train Loss: 0.7460 | Val Loss: 0.6652 | Val F1: 0.0435 | Val P: 0.1579 | Val R: 0.0252\n",
      "2026-02-28 23:34:18,630 INFO:\tStep 210 | Train Loss: 0.6260 | Val Loss: 0.6701 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-02-28 23:35:07,031 INFO:\tEpoch 2/12\n",
      "2026-02-28 23:37:19,345 INFO:\tStep 245 | Train Loss: 0.7446 | Val Loss: 0.6684 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-02-28 23:40:23,667 INFO:\tStep 280 | Train Loss: 0.6279 | Val Loss: 0.6804 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-02-28 23:43:28,208 INFO:\tStep 315 | Train Loss: 0.6614 | Val Loss: 0.6716 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-02-28 23:46:32,448 INFO:\tStep 350 | Train Loss: 0.6959 | Val Loss: 0.6898 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-02-28 23:49:36,458 INFO:\tStep 385 | Train Loss: 0.6712 | Val Loss: 0.6614 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-02-28 23:52:40,302 INFO:\tStep 420 | Train Loss: 0.6654 | Val Loss: 0.7121 | Val F1: 0.2593 | Val P: 0.2318 | Val R: 0.2941\n",
      "2026-02-28 23:54:21,786 INFO:\tEpoch 3/12\n",
      "2026-02-28 23:55:42,208 INFO:\tStep 455 | Train Loss: 0.6630 | Val Loss: 0.6658 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-02-28 23:58:50,664 INFO:\tStep 490 | Train Loss: 0.6227 | Val Loss: 0.6787 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 00:02:04,063 INFO:\tStep 525 | Train Loss: 0.6260 | Val Loss: 0.6886 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 00:05:17,539 INFO:\tStep 560 | Train Loss: 0.7000 | Val Loss: 0.6675 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 00:08:31,172 INFO:\tStep 595 | Train Loss: 0.7152 | Val Loss: 0.6654 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 00:11:44,758 INFO:\tStep 630 | Train Loss: 0.7465 | Val Loss: 0.6688 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 00:14:58,207 INFO:\tStep 665 | Train Loss: 0.6306 | Val Loss: 0.6722 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 00:15:15,760 INFO:\tEpoch 4/12\n",
      "2026-03-01 00:18:07,718 INFO:\tStep 700 | Train Loss: 0.6834 | Val Loss: 0.6582 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 00:21:21,197 INFO:\tStep 735 | Train Loss: 0.5762 | Val Loss: 0.6298 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 00:24:34,834 INFO:\tStep 770 | Train Loss: 0.6904 | Val Loss: 0.6759 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 00:27:48,368 INFO:\tStep 805 | Train Loss: 0.6392 | Val Loss: 0.6653 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 00:31:01,921 INFO:\tStep 840 | Train Loss: 0.6903 | Val Loss: 0.6599 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 00:34:15,671 INFO:\tStep 875 | Train Loss: 0.6657 | Val Loss: 0.5818 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 00:35:27,803 INFO:\tEpoch 5/12\n",
      "2026-03-01 00:37:24,942 INFO:\tStep 910 | Train Loss: 0.6381 | Val Loss: 0.6377 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 00:40:38,376 INFO:\tStep 945 | Train Loss: 0.5996 | Val Loss: 0.7121 | Val F1: 0.2861 | Val P: 0.1728 | Val R: 0.8319\n",
      "2026-03-01 00:43:52,845 INFO:\tStep 980 | Train Loss: 0.6490 | Val Loss: 0.6319 | Val F1: 0.1690 | Val P: 0.5217 | Val R: 0.1008\n",
      "2026-03-01 00:47:06,547 INFO:\tStep 1015 | Train Loss: 0.5664 | Val Loss: 0.5542 | Val F1: 0.3819 | Val P: 0.2724 | Val R: 0.6387\n",
      "2026-03-01 00:50:20,898 INFO:\tStep 1050 | Train Loss: 0.6046 | Val Loss: 0.5215 | Val F1: 0.3942 | Val P: 0.4607 | Val R: 0.3445\n",
      "2026-03-01 00:53:35,131 INFO:\tStep 1085 | Train Loss: 0.5351 | Val Loss: 0.5668 | Val F1: 0.4312 | Val P: 0.3867 | Val R: 0.4874\n",
      "2026-03-01 00:55:43,082 INFO:\tEpoch 6/12\n",
      "2026-03-01 00:56:45,722 INFO:\tStep 1120 | Train Loss: 0.5850 | Val Loss: 0.6512 | Val F1: 0.1944 | Val P: 0.5600 | Val R: 0.1176\n",
      "2026-03-01 00:59:59,793 INFO:\tStep 1155 | Train Loss: 0.5286 | Val Loss: 0.8253 | Val F1: 0.2133 | Val P: 0.5161 | Val R: 0.1345\n",
      "2026-03-01 01:03:13,726 INFO:\tStep 1190 | Train Loss: 0.5666 | Val Loss: 0.5529 | Val F1: 0.3906 | Val P: 0.2830 | Val R: 0.6303\n",
      "2026-03-01 01:06:27,644 INFO:\tStep 1225 | Train Loss: 0.5591 | Val Loss: 0.5502 | Val F1: 0.3077 | Val P: 0.4444 | Val R: 0.2353\n",
      "2026-03-01 01:09:41,573 INFO:\tStep 1260 | Train Loss: 0.5528 | Val Loss: 0.5766 | Val F1: 0.3568 | Val P: 0.4043 | Val R: 0.3193\n",
      "2026-03-01 01:12:55,243 INFO:\tStep 1295 | Train Loss: 0.6123 | Val Loss: 0.5577 | Val F1: 0.3902 | Val P: 0.4651 | Val R: 0.3361\n",
      "2026-03-01 01:16:09,054 INFO:\tStep 1330 | Train Loss: 0.5399 | Val Loss: 0.7335 | Val F1: 0.3671 | Val P: 0.2451 | Val R: 0.7311\n",
      "2026-03-01 01:16:48,275 INFO:\tEpoch 7/12\n",
      "2026-03-01 01:19:18,592 INFO:\tStep 1365 | Train Loss: 0.5541 | Val Loss: 0.5259 | Val F1: 0.4182 | Val P: 0.3270 | Val R: 0.5798\n",
      "2026-03-01 01:22:31,846 INFO:\tStep 1400 | Train Loss: 0.4778 | Val Loss: 0.5173 | Val F1: 0.4415 | Val P: 0.3667 | Val R: 0.5546\n",
      "2026-03-01 01:25:46,226 INFO:\tStep 1435 | Train Loss: 0.4584 | Val Loss: 0.5205 | Val F1: 0.4437 | Val P: 0.3818 | Val R: 0.5294\n",
      "2026-03-01 01:29:00,854 INFO:\tStep 1470 | Train Loss: 0.4973 | Val Loss: 0.5427 | Val F1: 0.4231 | Val P: 0.3901 | Val R: 0.4622\n",
      "2026-03-01 01:32:14,313 INFO:\tStep 1505 | Train Loss: 0.4987 | Val Loss: 0.5309 | Val F1: 0.4216 | Val P: 0.3037 | Val R: 0.6891\n",
      "2026-03-01 01:35:28,242 INFO:\tStep 1540 | Train Loss: 0.4891 | Val Loss: 0.5101 | Val F1: 0.4048 | Val P: 0.3835 | Val R: 0.4286\n",
      "2026-03-01 01:37:02,229 INFO:\tEpoch 8/12\n",
      "2026-03-01 01:38:37,887 INFO:\tStep 1575 | Train Loss: 0.5468 | Val Loss: 0.4842 | Val F1: 0.3670 | Val P: 0.4040 | Val R: 0.3361\n",
      "2026-03-01 01:41:51,947 INFO:\tStep 1610 | Train Loss: 0.4896 | Val Loss: 0.5285 | Val F1: 0.4270 | Val P: 0.3147 | Val R: 0.6639\n",
      "2026-03-01 01:45:05,802 INFO:\tStep 1645 | Train Loss: 0.4549 | Val Loss: 0.5062 | Val F1: 0.4324 | Val P: 0.3364 | Val R: 0.6050\n",
      "2026-03-01 01:48:19,568 INFO:\tStep 1680 | Train Loss: 0.4388 | Val Loss: 0.4866 | Val F1: 0.4551 | Val P: 0.3535 | Val R: 0.6387\n",
      "2026-03-01 01:51:34,173 INFO:\tStep 1715 | Train Loss: 0.4311 | Val Loss: 0.5202 | Val F1: 0.4380 | Val P: 0.3871 | Val R: 0.5042\n",
      "2026-03-01 01:54:47,848 INFO:\tStep 1750 | Train Loss: 0.3946 | Val Loss: 0.5160 | Val F1: 0.4270 | Val P: 0.3207 | Val R: 0.6387\n",
      "2026-03-01 01:58:01,365 INFO:\tStep 1785 | Train Loss: 0.4602 | Val Loss: 0.5392 | Val F1: 0.3982 | Val P: 0.4314 | Val R: 0.3697\n",
      "2026-03-01 01:58:07,519 INFO:\tEpoch 9/12\n",
      "2026-03-01 02:01:10,804 INFO:\tStep 1820 | Train Loss: 0.3810 | Val Loss: 0.5385 | Val F1: 0.4422 | Val P: 0.3641 | Val R: 0.5630\n",
      "2026-03-01 02:04:24,293 INFO:\tStep 1855 | Train Loss: 0.4013 | Val Loss: 0.5479 | Val F1: 0.3982 | Val P: 0.4314 | Val R: 0.3697\n",
      "2026-03-01 02:07:37,788 INFO:\tStep 1890 | Train Loss: 0.5533 | Val Loss: 0.5162 | Val F1: 0.3805 | Val P: 0.4535 | Val R: 0.3277\n",
      "2026-03-01 02:10:51,217 INFO:\tStep 1925 | Train Loss: 0.4360 | Val Loss: 0.4878 | Val F1: 0.4658 | Val P: 0.3931 | Val R: 0.5714\n",
      "2026-03-01 02:14:05,392 INFO:\tStep 1960 | Train Loss: 0.4582 | Val Loss: 0.4911 | Val F1: 0.4387 | Val P: 0.3560 | Val R: 0.5714\n",
      "2026-03-01 02:17:19,100 INFO:\tStep 1995 | Train Loss: 0.4334 | Val Loss: 0.4920 | Val F1: 0.4429 | Val P: 0.3765 | Val R: 0.5378\n",
      "2026-03-01 02:18:20,146 INFO:\tEpoch 10/12\n",
      "2026-03-01 02:20:29,124 INFO:\tStep 2030 | Train Loss: 0.4126 | Val Loss: 0.5100 | Val F1: 0.4215 | Val P: 0.4146 | Val R: 0.4286\n",
      "2026-03-01 02:23:42,756 INFO:\tStep 2065 | Train Loss: 0.4380 | Val Loss: 0.4852 | Val F1: 0.4017 | Val P: 0.4000 | Val R: 0.4034\n",
      "2026-03-01 02:26:56,900 INFO:\tStep 2100 | Train Loss: 0.4179 | Val Loss: 0.4873 | Val F1: 0.4148 | Val P: 0.3709 | Val R: 0.4706\n",
      "2026-03-01 02:30:10,490 INFO:\tStep 2135 | Train Loss: 0.3538 | Val Loss: 0.4981 | Val F1: 0.4304 | Val P: 0.4322 | Val R: 0.4286\n",
      "2026-03-01 02:33:24,410 INFO:\tStep 2170 | Train Loss: 0.4357 | Val Loss: 0.4819 | Val F1: 0.4306 | Val P: 0.3669 | Val R: 0.5210\n",
      "2026-03-01 02:36:38,135 INFO:\tStep 2205 | Train Loss: 0.4096 | Val Loss: 0.4977 | Val F1: 0.4444 | Val P: 0.3458 | Val R: 0.6218\n",
      "2026-03-01 02:38:33,842 INFO:\tEpoch 11/12\n",
      "2026-03-01 02:39:47,905 INFO:\tStep 2240 | Train Loss: 0.3840 | Val Loss: 0.4942 | Val F1: 0.4522 | Val P: 0.3451 | Val R: 0.6555\n",
      "2026-03-01 02:43:01,651 INFO:\tStep 2275 | Train Loss: 0.3531 | Val Loss: 0.4970 | Val F1: 0.4656 | Val P: 0.3817 | Val R: 0.5966\n",
      "2026-03-01 02:46:15,389 INFO:\tStep 2310 | Train Loss: 0.4223 | Val Loss: 0.4908 | Val F1: 0.4573 | Val P: 0.3589 | Val R: 0.6303\n",
      "2026-03-01 02:49:28,690 INFO:\tStep 2345 | Train Loss: 0.4026 | Val Loss: 0.4925 | Val F1: 0.4539 | Val P: 0.3730 | Val R: 0.5798\n",
      "2026-03-01 02:52:41,852 INFO:\tStep 2380 | Train Loss: 0.4028 | Val Loss: 0.4812 | Val F1: 0.4626 | Val P: 0.3886 | Val R: 0.5714\n",
      "2026-03-01 02:55:55,457 INFO:\tStep 2415 | Train Loss: 0.3930 | Val Loss: 0.4833 | Val F1: 0.4275 | Val P: 0.3758 | Val R: 0.4958\n",
      "2026-03-01 02:59:09,187 INFO:\tStep 2450 | Train Loss: 0.3942 | Val Loss: 0.4827 | Val F1: 0.4332 | Val P: 0.3797 | Val R: 0.5042\n",
      "2026-03-01 02:59:36,977 INFO:\tEpoch 12/12\n",
      "2026-03-01 03:02:18,384 INFO:\tStep 2485 | Train Loss: 0.3896 | Val Loss: 0.4827 | Val F1: 0.4406 | Val P: 0.3772 | Val R: 0.5294\n",
      "2026-03-01 03:05:31,865 INFO:\tStep 2520 | Train Loss: 0.4818 | Val Loss: 0.4802 | Val F1: 0.4646 | Val P: 0.3876 | Val R: 0.5798\n",
      "2026-03-01 03:08:45,382 INFO:\tStep 2555 | Train Loss: 0.3519 | Val Loss: 0.4793 | Val F1: 0.4718 | Val P: 0.3901 | Val R: 0.5966\n",
      "2026-03-01 03:12:00,400 INFO:\tStep 2590 | Train Loss: 0.3451 | Val Loss: 0.4791 | Val F1: 0.4631 | Val P: 0.3855 | Val R: 0.5798\n",
      "2026-03-01 03:15:13,877 INFO:\tStep 2625 | Train Loss: 0.3634 | Val Loss: 0.4793 | Val F1: 0.4631 | Val P: 0.3855 | Val R: 0.5798\n",
      "2026-03-01 03:18:27,555 INFO:\tStep 2660 | Train Loss: 0.3846 | Val Loss: 0.4793 | Val F1: 0.4631 | Val P: 0.3855 | Val R: 0.5798\n",
      "2026-03-01 03:20:42,183 INFO:\tOptimal threshold: 0.450 (val F1 with threshold: 0.4728, val F1 @0.5: 0.4718)\n",
      "2026-03-01 03:22:07,198 INFO:\tDev Loss: 0.4903 | Dev F1: 0.4586 | Dev P: 0.3664 | Dev R: 0.6131 | Threshold: 0.450\n",
      "2026-03-01 03:22:11,480 INFO:\t[F_large] New best saved (val F1=0.4728)\n",
      "\u001b[32m[I 2026-03-01 03:22:11,988]\u001b[0m Trial 0 finished with value: 0.4728434504792332 and parameters: {'lr': 4.737727900728161e-06, 'hidden_dim': 0, 'weight_decay': 0.0010401663679887319, 'head_lr_multiplier': 3}. Best is trial 0 with value: 0.4728434504792332.\u001b[0m\n",
      "2026-03-01 03:22:11,991 INFO:\t[F_large] Trial 1: lr=1.47e-05, pool=PoolingStrategy.CLS_MEAN, hidden=256, wd=4.4e-03, head_lr_mult=3\n",
      "2026-03-01 03:22:14,569 INFO:\tHTTP Request: HEAD https://huggingface.co/microsoft/deberta-v3-large/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-03-01 03:22:14,577 INFO:\tHTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/microsoft/deberta-v3-large/64a8c8eab3e352a784c658aef62be1662607476f/config.json \"HTTP/1.1 200 OK\"\n",
      "2026-03-01 03:22:14,671 INFO:\tHTTP Request: HEAD https://huggingface.co/microsoft/deberta-v3-large/resolve/main/model.safetensors \"HTTP/1.1 404 Not Found\"\n",
      "2026-03-01 03:22:14,770 INFO:\tHTTP Request: GET https://huggingface.co/api/models/microsoft/deberta-v3-large \"HTTP/1.1 200 OK\"\n",
      "2026-03-01 03:22:14,882 INFO:\tHTTP Request: GET https://huggingface.co/api/models/microsoft/deberta-v3-large/commits/main \"HTTP/1.1 200 OK\"\n",
      "2026-03-01 03:22:14,995 INFO:\tHTTP Request: GET https://huggingface.co/api/models/microsoft/deberta-v3-large/discussions?p=0 \"HTTP/1.1 200 OK\"\n",
      "2026-03-01 03:22:15,111 INFO:\tHTTP Request: GET https://huggingface.co/api/models/microsoft/deberta-v3-large/commits/refs%2Fpr%2F13 \"HTTP/1.1 200 OK\"\n",
      "2026-03-01 03:22:15,209 INFO:\tHTTP Request: HEAD https://huggingface.co/microsoft/deberta-v3-large/resolve/refs%2Fpr%2F13/model.safetensors.index.json \"HTTP/1.1 404 Not Found\"\n",
      "2026-03-01 03:22:15,303 INFO:\tHTTP Request: HEAD https://huggingface.co/microsoft/deberta-v3-large/resolve/refs%2Fpr%2F13/model.safetensors \"HTTP/1.1 302 Found\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2cf6e4e974f4e3db5e8878921638df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mDebertaV2Model LOAD REPORT\u001b[0m from: microsoft/deberta-v3-large\n",
      "Key                                     | Status     |  | \n",
      "----------------------------------------+------------+--+-\n",
      "mask_predictions.dense.weight           | UNEXPECTED |  | \n",
      "mask_predictions.dense.bias             | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED |  | \n",
      "mask_predictions.classifier.weight      | UNEXPECTED |  | \n",
      "mask_predictions.LayerNorm.weight       | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED |  | \n",
      "mask_predictions.LayerNorm.bias         | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED |  | \n",
      "mask_predictions.classifier.bias        | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "2026-03-01 03:22:16,840 INFO:\tBackbone model loaded: DebertaV2Model, dtype torch.float32, gradient_checkpointing=False\n",
      "2026-03-01 03:22:17,474 INFO:\tpos_weight = 3.09 (raw ratio=9.55, neg=6443, pos=675)\n",
      "2026-03-01 03:22:17,478 INFO:\tUsing bitsandbytes 8-bit AdamW\n",
      "2026-03-01 03:22:17,479 INFO:\tEarly stopping: patience=4 epochs = 24 eval rounds (6 evals/epoch, eval every 35 steps)\n",
      "2026-03-01 03:22:17,480 INFO:\tGradient accumulation: 16 micro-steps per optimizer step\n",
      "2026-03-01 03:22:17,485 INFO:\tEpoch 1/12\n",
      "2026-03-01 03:25:30,016 INFO:\tStep 35 | Train Loss: 0.7483 | Val Loss: 0.6696 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 03:28:36,833 INFO:\tStep 70 | Train Loss: 0.6883 | Val Loss: 0.7330 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 03:31:43,373 INFO:\tStep 105 | Train Loss: 0.6955 | Val Loss: 0.6661 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 03:34:49,728 INFO:\tStep 140 | Train Loss: 0.7261 | Val Loss: 0.6665 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 03:37:55,999 INFO:\tStep 175 | Train Loss: 0.6834 | Val Loss: 0.6703 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 03:41:02,449 INFO:\tStep 210 | Train Loss: 0.6851 | Val Loss: 0.6659 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 03:41:51,381 INFO:\tEpoch 2/12\n",
      "2026-03-01 03:44:05,170 INFO:\tStep 245 | Train Loss: 0.6334 | Val Loss: 0.6676 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 03:47:11,646 INFO:\tStep 280 | Train Loss: 0.6537 | Val Loss: 0.6783 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 03:50:18,240 INFO:\tStep 315 | Train Loss: 0.7003 | Val Loss: 0.6670 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 03:53:24,656 INFO:\tStep 350 | Train Loss: 0.6910 | Val Loss: 0.6695 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 03:56:31,038 INFO:\tStep 385 | Train Loss: 0.6703 | Val Loss: 0.6828 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 03:59:37,497 INFO:\tStep 420 | Train Loss: 0.6726 | Val Loss: 0.6679 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 04:01:18,952 INFO:\tEpoch 3/12\n",
      "2026-03-01 04:02:40,011 INFO:\tStep 455 | Train Loss: 0.6741 | Val Loss: 0.6720 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 04:05:46,655 INFO:\tStep 490 | Train Loss: 0.6841 | Val Loss: 0.6667 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 04:08:53,317 INFO:\tStep 525 | Train Loss: 0.6744 | Val Loss: 0.6706 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 04:12:00,023 INFO:\tStep 560 | Train Loss: 0.6687 | Val Loss: 0.6692 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 04:15:06,561 INFO:\tStep 595 | Train Loss: 0.7101 | Val Loss: 0.6655 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 04:18:12,999 INFO:\tStep 630 | Train Loss: 0.6728 | Val Loss: 0.6723 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 04:21:19,516 INFO:\tStep 665 | Train Loss: 0.6384 | Val Loss: 0.6827 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 04:21:36,607 INFO:\tEpoch 4/12\n",
      "2026-03-01 04:24:22,304 INFO:\tStep 700 | Train Loss: 0.6419 | Val Loss: 0.6701 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 04:27:28,726 INFO:\tStep 735 | Train Loss: 0.6454 | Val Loss: 0.6656 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 04:30:35,204 INFO:\tStep 770 | Train Loss: 0.6665 | Val Loss: 0.6665 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 04:33:41,318 INFO:\tStep 805 | Train Loss: 0.7265 | Val Loss: 0.6657 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 04:36:47,550 INFO:\tStep 840 | Train Loss: 0.6646 | Val Loss: 0.6676 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 04:39:53,862 INFO:\tStep 875 | Train Loss: 0.6792 | Val Loss: 0.6662 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 04:39:53,863 INFO:\tEarly stopping at step 875\n",
      "2026-03-01 04:40:42,770 INFO:\tOptimal threshold: 0.250 (val F1 with threshold: 0.1730, val F1 @0.5: 0.0000)\n",
      "2026-03-01 04:42:04,795 INFO:\tDev Loss: 0.6697 | Dev F1: 0.1736 | Dev P: 0.0951 | Dev R: 1.0000 | Threshold: 0.250\n",
      "\u001b[32m[I 2026-03-01 04:42:05,233]\u001b[0m Trial 1 finished with value: 0.17296511627906977 and parameters: {'lr': 1.4696236810540886e-05, 'hidden_dim': 256, 'dropout_rate': 0.1, 'weight_decay': 0.0044447541666908135, 'head_lr_multiplier': 3}. Best is trial 0 with value: 0.4728434504792332.\u001b[0m\n",
      "2026-03-01 04:42:05,236 INFO:\t[F_large] Trial 2: lr=3.05e-06, pool=PoolingStrategy.CLS_MEAN, hidden=256, wd=3.1e-04, head_lr_mult=3\n",
      "2026-03-01 04:42:07,726 INFO:\tHTTP Request: HEAD https://huggingface.co/microsoft/deberta-v3-large/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-03-01 04:42:07,735 INFO:\tHTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/microsoft/deberta-v3-large/64a8c8eab3e352a784c658aef62be1662607476f/config.json \"HTTP/1.1 200 OK\"\n",
      "2026-03-01 04:42:07,855 INFO:\tHTTP Request: HEAD https://huggingface.co/microsoft/deberta-v3-large/resolve/main/model.safetensors \"HTTP/1.1 404 Not Found\"\n",
      "2026-03-01 04:42:07,951 INFO:\tHTTP Request: GET https://huggingface.co/api/models/microsoft/deberta-v3-large \"HTTP/1.1 200 OK\"\n",
      "2026-03-01 04:42:08,062 INFO:\tHTTP Request: GET https://huggingface.co/api/models/microsoft/deberta-v3-large/commits/main \"HTTP/1.1 200 OK\"\n",
      "2026-03-01 04:42:08,177 INFO:\tHTTP Request: GET https://huggingface.co/api/models/microsoft/deberta-v3-large/discussions?p=0 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e077e8b14e24893b662287c03629bc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-03-01 04:42:08,296 INFO:\tHTTP Request: GET https://huggingface.co/api/models/microsoft/deberta-v3-large/commits/refs%2Fpr%2F13 \"HTTP/1.1 200 OK\"\n",
      "\u001b[1mDebertaV2Model LOAD REPORT\u001b[0m from: microsoft/deberta-v3-large\n",
      "Key                                     | Status     |  | \n",
      "----------------------------------------+------------+--+-\n",
      "mask_predictions.dense.weight           | UNEXPECTED |  | \n",
      "mask_predictions.dense.bias             | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED |  | \n",
      "mask_predictions.classifier.weight      | UNEXPECTED |  | \n",
      "mask_predictions.LayerNorm.weight       | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED |  | \n",
      "mask_predictions.LayerNorm.bias         | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED |  | \n",
      "mask_predictions.classifier.bias        | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "2026-03-01 04:42:08,608 INFO:\tHTTP Request: HEAD https://huggingface.co/microsoft/deberta-v3-large/resolve/refs%2Fpr%2F13/model.safetensors.index.json \"HTTP/1.1 404 Not Found\"\n",
      "2026-03-01 04:42:08,703 INFO:\tHTTP Request: HEAD https://huggingface.co/microsoft/deberta-v3-large/resolve/refs%2Fpr%2F13/model.safetensors \"HTTP/1.1 302 Found\"\n",
      "2026-03-01 04:42:09,045 INFO:\tBackbone model loaded: DebertaV2Model, dtype torch.float32, gradient_checkpointing=False\n",
      "2026-03-01 04:42:09,588 INFO:\tpos_weight = 3.09 (raw ratio=9.55, neg=6443, pos=675)\n",
      "2026-03-01 04:42:09,591 INFO:\tUsing bitsandbytes 8-bit AdamW\n",
      "2026-03-01 04:42:09,592 INFO:\tEarly stopping: patience=4 epochs = 24 eval rounds (6 evals/epoch, eval every 35 steps)\n",
      "2026-03-01 04:42:09,593 INFO:\tGradient accumulation: 16 micro-steps per optimizer step\n",
      "2026-03-01 04:42:09,595 INFO:\tEpoch 1/12\n",
      "2026-03-01 04:45:18,787 INFO:\tStep 35 | Train Loss: 0.8250 | Val Loss: 0.7974 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 04:48:27,550 INFO:\tStep 70 | Train Loss: 0.7708 | Val Loss: 0.7283 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 04:51:36,018 INFO:\tStep 105 | Train Loss: 0.6748 | Val Loss: 0.6676 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 04:54:44,704 INFO:\tStep 140 | Train Loss: 0.7248 | Val Loss: 0.6669 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 04:57:53,377 INFO:\tStep 175 | Train Loss: 0.6403 | Val Loss: 0.6698 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 05:01:02,245 INFO:\tStep 210 | Train Loss: 0.6648 | Val Loss: 0.6668 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 05:01:51,695 INFO:\tEpoch 2/12\n",
      "2026-03-01 05:04:07,092 INFO:\tStep 245 | Train Loss: 0.6774 | Val Loss: 0.6642 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 05:07:15,852 INFO:\tStep 280 | Train Loss: 0.6536 | Val Loss: 0.6745 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 05:10:24,809 INFO:\tStep 315 | Train Loss: 0.6705 | Val Loss: 0.6672 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 05:13:33,528 INFO:\tStep 350 | Train Loss: 0.6891 | Val Loss: 0.6687 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 05:16:42,120 INFO:\tStep 385 | Train Loss: 0.6356 | Val Loss: 0.6702 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 05:19:50,917 INFO:\tStep 420 | Train Loss: 0.6840 | Val Loss: 0.6662 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 05:21:33,642 INFO:\tEpoch 3/12\n",
      "2026-03-01 05:22:55,645 INFO:\tStep 455 | Train Loss: 0.6780 | Val Loss: 0.6702 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 05:26:04,672 INFO:\tStep 490 | Train Loss: 0.7280 | Val Loss: 0.6667 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 05:29:13,535 INFO:\tStep 525 | Train Loss: 0.6541 | Val Loss: 0.6774 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 05:32:21,865 INFO:\tStep 560 | Train Loss: 0.7091 | Val Loss: 0.6660 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 05:35:30,805 INFO:\tStep 595 | Train Loss: 0.6599 | Val Loss: 0.6874 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 05:38:39,969 INFO:\tStep 630 | Train Loss: 0.6425 | Val Loss: 0.6714 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 05:41:48,317 INFO:\tStep 665 | Train Loss: 0.6545 | Val Loss: 0.6712 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 05:42:05,454 INFO:\tEpoch 4/12\n",
      "2026-03-01 05:44:53,254 INFO:\tStep 700 | Train Loss: 0.7338 | Val Loss: 0.6658 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 05:48:02,046 INFO:\tStep 735 | Train Loss: 0.6185 | Val Loss: 0.6758 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 05:51:11,068 INFO:\tStep 770 | Train Loss: 0.6469 | Val Loss: 0.6683 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 05:54:19,699 INFO:\tStep 805 | Train Loss: 0.6781 | Val Loss: 0.6644 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 05:57:27,858 INFO:\tStep 840 | Train Loss: 0.6844 | Val Loss: 0.6646 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 06:00:36,454 INFO:\tStep 875 | Train Loss: 0.6559 | Val Loss: 0.6663 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 06:00:36,455 INFO:\tEarly stopping at step 875\n",
      "2026-03-01 06:01:25,946 INFO:\tOptimal threshold: 0.500 (val F1 with threshold: 0.0000, val F1 @0.5: 0.0000)\n",
      "2026-03-01 06:02:48,661 INFO:\tDev Loss: 0.6702 | Dev F1: 0.0000 | Dev P: 0.0000 | Dev R: 0.0000 | Threshold: 0.500\n",
      "\u001b[32m[I 2026-03-01 06:02:49,139]\u001b[0m Trial 2 finished with value: 0.0 and parameters: {'lr': 3.050945891610522e-06, 'hidden_dim': 256, 'dropout_rate': 0.2, 'weight_decay': 0.0003124565071260876, 'head_lr_multiplier': 3}. Best is trial 0 with value: 0.4728434504792332.\u001b[0m\n",
      "2026-03-01 06:02:49,141 INFO:\t[F_large] Trial 3: lr=4.65e-06, pool=PoolingStrategy.CLS_MEAN, hidden=256, wd=7.5e-04, head_lr_mult=10\n",
      "2026-03-01 06:02:51,658 INFO:\tHTTP Request: HEAD https://huggingface.co/microsoft/deberta-v3-large/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-03-01 06:02:51,667 INFO:\tHTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/microsoft/deberta-v3-large/64a8c8eab3e352a784c658aef62be1662607476f/config.json \"HTTP/1.1 200 OK\"\n",
      "2026-03-01 06:02:51,762 INFO:\tHTTP Request: HEAD https://huggingface.co/microsoft/deberta-v3-large/resolve/main/model.safetensors \"HTTP/1.1 404 Not Found\"\n",
      "2026-03-01 06:02:51,859 INFO:\tHTTP Request: GET https://huggingface.co/api/models/microsoft/deberta-v3-large \"HTTP/1.1 200 OK\"\n",
      "2026-03-01 06:02:51,973 INFO:\tHTTP Request: GET https://huggingface.co/api/models/microsoft/deberta-v3-large/commits/main \"HTTP/1.1 200 OK\"\n",
      "2026-03-01 06:02:52,082 INFO:\tHTTP Request: GET https://huggingface.co/api/models/microsoft/deberta-v3-large/discussions?p=0 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55cf110be580425593187fe6945c9e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-03-01 06:02:52,193 INFO:\tHTTP Request: GET https://huggingface.co/api/models/microsoft/deberta-v3-large/commits/refs%2Fpr%2F13 \"HTTP/1.1 200 OK\"\n",
      "\u001b[1mDebertaV2Model LOAD REPORT\u001b[0m from: microsoft/deberta-v3-large\n",
      "Key                                     | Status     |  | \n",
      "----------------------------------------+------------+--+-\n",
      "mask_predictions.dense.weight           | UNEXPECTED |  | \n",
      "mask_predictions.dense.bias             | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED |  | \n",
      "mask_predictions.classifier.weight      | UNEXPECTED |  | \n",
      "mask_predictions.LayerNorm.weight       | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED |  | \n",
      "mask_predictions.LayerNorm.bias         | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED |  | \n",
      "mask_predictions.classifier.bias        | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "2026-03-01 06:02:52,518 INFO:\tHTTP Request: HEAD https://huggingface.co/microsoft/deberta-v3-large/resolve/refs%2Fpr%2F13/model.safetensors.index.json \"HTTP/1.1 404 Not Found\"\n",
      "2026-03-01 06:02:52,615 INFO:\tHTTP Request: HEAD https://huggingface.co/microsoft/deberta-v3-large/resolve/refs%2Fpr%2F13/model.safetensors \"HTTP/1.1 302 Found\"\n",
      "2026-03-01 06:02:52,814 INFO:\tBackbone model loaded: DebertaV2Model, dtype torch.float32, gradient_checkpointing=False\n",
      "2026-03-01 06:02:53,430 INFO:\tpos_weight = 3.09 (raw ratio=9.55, neg=6443, pos=675)\n",
      "2026-03-01 06:02:53,434 INFO:\tUsing bitsandbytes 8-bit AdamW\n",
      "2026-03-01 06:02:53,435 INFO:\tEarly stopping: patience=4 epochs = 24 eval rounds (6 evals/epoch, eval every 35 steps)\n",
      "2026-03-01 06:02:53,435 INFO:\tGradient accumulation: 16 micro-steps per optimizer step\n",
      "2026-03-01 06:02:53,439 INFO:\tEpoch 1/12\n",
      "2026-03-01 06:06:02,492 INFO:\tStep 35 | Train Loss: 0.8703 | Val Loss: 0.7539 | Val F1: 0.0136 | Val P: 0.0357 | Val R: 0.0084\n",
      "2026-03-01 06:09:12,455 INFO:\tStep 70 | Train Loss: 0.6747 | Val Loss: 0.6707 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 06:12:21,271 INFO:\tStep 105 | Train Loss: 0.6767 | Val Loss: 0.6765 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 06:15:29,795 INFO:\tStep 140 | Train Loss: 0.7064 | Val Loss: 0.6772 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 06:18:38,737 INFO:\tStep 175 | Train Loss: 0.6701 | Val Loss: 0.6740 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 06:21:47,632 INFO:\tStep 210 | Train Loss: 0.7031 | Val Loss: 0.6905 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 06:22:36,976 INFO:\tEpoch 2/12\n",
      "2026-03-01 06:24:52,677 INFO:\tStep 245 | Train Loss: 0.6486 | Val Loss: 0.7008 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 06:28:01,422 INFO:\tStep 280 | Train Loss: 0.6709 | Val Loss: 0.6896 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 06:31:09,781 INFO:\tStep 315 | Train Loss: 0.7178 | Val Loss: 0.6762 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 06:34:18,677 INFO:\tStep 350 | Train Loss: 0.7042 | Val Loss: 0.6746 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 06:37:27,267 INFO:\tStep 385 | Train Loss: 0.6443 | Val Loss: 0.6661 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 06:40:36,191 INFO:\tStep 420 | Train Loss: 0.6919 | Val Loss: 0.6609 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 06:42:18,954 INFO:\tEpoch 3/12\n",
      "2026-03-01 06:43:41,185 INFO:\tStep 455 | Train Loss: 0.6760 | Val Loss: 0.6672 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 06:46:50,154 INFO:\tStep 490 | Train Loss: 0.6721 | Val Loss: 0.6965 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 06:49:58,988 INFO:\tStep 525 | Train Loss: 0.6837 | Val Loss: 0.6678 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 06:53:07,850 INFO:\tStep 560 | Train Loss: 0.6398 | Val Loss: 0.6698 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 06:56:16,515 INFO:\tStep 595 | Train Loss: 0.6863 | Val Loss: 0.6649 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 06:59:25,505 INFO:\tStep 630 | Train Loss: 0.6980 | Val Loss: 0.6695 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 07:02:34,216 INFO:\tStep 665 | Train Loss: 0.6318 | Val Loss: 0.6663 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 07:02:51,323 INFO:\tEpoch 4/12\n",
      "2026-03-01 07:05:39,114 INFO:\tStep 700 | Train Loss: 0.6736 | Val Loss: 0.6609 | Val F1: 0.2871 | Val P: 0.2035 | Val R: 0.4874\n",
      "2026-03-01 07:08:50,278 INFO:\tStep 735 | Train Loss: 0.5738 | Val Loss: 0.6491 | Val F1: 0.3012 | Val P: 0.5319 | Val R: 0.2101\n",
      "2026-03-01 07:12:00,091 INFO:\tStep 770 | Train Loss: 0.6397 | Val Loss: 0.5793 | Val F1: 0.3021 | Val P: 0.2358 | Val R: 0.4202\n",
      "2026-03-01 07:15:09,844 INFO:\tStep 805 | Train Loss: 0.6006 | Val Loss: 0.5809 | Val F1: 0.2745 | Val P: 0.2059 | Val R: 0.4118\n",
      "2026-03-01 07:18:18,708 INFO:\tStep 840 | Train Loss: 0.6238 | Val Loss: 0.5519 | Val F1: 0.3821 | Val P: 0.2656 | Val R: 0.6807\n",
      "2026-03-01 07:21:28,376 INFO:\tStep 875 | Train Loss: 0.6097 | Val Loss: 0.5757 | Val F1: 0.4061 | Val P: 0.3175 | Val R: 0.5630\n",
      "2026-03-01 07:22:39,923 INFO:\tEpoch 5/12\n",
      "2026-03-01 07:24:34,572 INFO:\tStep 910 | Train Loss: 0.5703 | Val Loss: 0.5594 | Val F1: 0.4178 | Val P: 0.3030 | Val R: 0.6723\n",
      "2026-03-01 07:27:44,044 INFO:\tStep 945 | Train Loss: 0.5545 | Val Loss: 0.5784 | Val F1: 0.3756 | Val P: 0.4255 | Val R: 0.3361\n",
      "2026-03-01 07:30:52,864 INFO:\tStep 980 | Train Loss: 0.5563 | Val Loss: 0.6444 | Val F1: 0.2667 | Val P: 0.4783 | Val R: 0.1849\n",
      "2026-03-01 07:34:01,567 INFO:\tStep 1015 | Train Loss: 0.5688 | Val Loss: 0.6143 | Val F1: 0.2329 | Val P: 0.6296 | Val R: 0.1429\n",
      "2026-03-01 07:37:10,748 INFO:\tStep 1050 | Train Loss: 0.5925 | Val Loss: 0.5822 | Val F1: 0.2368 | Val P: 0.5455 | Val R: 0.1513\n",
      "2026-03-01 07:40:19,811 INFO:\tStep 1085 | Train Loss: 0.6557 | Val Loss: 0.5914 | Val F1: 0.3386 | Val P: 0.4571 | Val R: 0.2689\n",
      "2026-03-01 07:42:23,806 INFO:\tEpoch 6/12\n",
      "2026-03-01 07:43:24,793 INFO:\tStep 1120 | Train Loss: 0.6043 | Val Loss: 0.5536 | Val F1: 0.4022 | Val P: 0.2972 | Val R: 0.6218\n",
      "2026-03-01 07:46:33,371 INFO:\tStep 1155 | Train Loss: 0.5413 | Val Loss: 0.6095 | Val F1: 0.3128 | Val P: 0.4667 | Val R: 0.2353\n",
      "2026-03-01 07:49:41,974 INFO:\tStep 1190 | Train Loss: 0.5528 | Val Loss: 0.5886 | Val F1: 0.3469 | Val P: 0.4416 | Val R: 0.2857\n",
      "2026-03-01 07:52:50,942 INFO:\tStep 1225 | Train Loss: 0.5377 | Val Loss: 0.5489 | Val F1: 0.3923 | Val P: 0.4556 | Val R: 0.3445\n",
      "2026-03-01 07:55:59,692 INFO:\tStep 1260 | Train Loss: 0.5108 | Val Loss: 0.5400 | Val F1: 0.3785 | Val P: 0.2480 | Val R: 0.7983\n",
      "2026-03-01 07:59:08,722 INFO:\tStep 1295 | Train Loss: 0.5370 | Val Loss: 0.4994 | Val F1: 0.4427 | Val P: 0.4056 | Val R: 0.4874\n",
      "2026-03-01 08:02:18,452 INFO:\tStep 1330 | Train Loss: 0.4897 | Val Loss: 0.5647 | Val F1: 0.4375 | Val P: 0.4667 | Val R: 0.4118\n",
      "2026-03-01 08:02:56,666 INFO:\tEpoch 7/12\n",
      "2026-03-01 08:05:23,219 INFO:\tStep 1365 | Train Loss: 0.4744 | Val Loss: 0.5084 | Val F1: 0.4444 | Val P: 0.3636 | Val R: 0.5714\n",
      "2026-03-01 08:08:32,783 INFO:\tStep 1400 | Train Loss: 0.4616 | Val Loss: 0.5192 | Val F1: 0.4476 | Val P: 0.3376 | Val R: 0.6639\n",
      "2026-03-01 08:11:42,514 INFO:\tStep 1435 | Train Loss: 0.5226 | Val Loss: 0.4965 | Val F1: 0.4248 | Val P: 0.4486 | Val R: 0.4034\n",
      "2026-03-01 08:14:51,670 INFO:\tStep 1470 | Train Loss: 0.4497 | Val Loss: 0.5271 | Val F1: 0.4268 | Val P: 0.3028 | Val R: 0.7227\n",
      "2026-03-01 08:18:00,696 INFO:\tStep 1505 | Train Loss: 0.4742 | Val Loss: 0.5254 | Val F1: 0.4356 | Val P: 0.4623 | Val R: 0.4118\n",
      "2026-03-01 08:21:09,705 INFO:\tStep 1540 | Train Loss: 0.4855 | Val Loss: 0.4883 | Val F1: 0.4639 | Val P: 0.3700 | Val R: 0.6218\n",
      "2026-03-01 08:22:42,237 INFO:\tEpoch 8/12\n",
      "2026-03-01 08:24:15,715 INFO:\tStep 1575 | Train Loss: 0.4351 | Val Loss: 0.5068 | Val F1: 0.4549 | Val P: 0.4265 | Val R: 0.4874\n",
      "2026-03-01 08:27:25,066 INFO:\tStep 1610 | Train Loss: 0.4899 | Val Loss: 0.5160 | Val F1: 0.4301 | Val P: 0.3750 | Val R: 0.5042\n",
      "2026-03-01 08:30:44,887 INFO:\tStep 1645 | Train Loss: 0.4930 | Val Loss: 0.5287 | Val F1: 0.4786 | Val P: 0.4870 | Val R: 0.4706\n",
      "2026-03-01 08:33:54,073 INFO:\tStep 1680 | Train Loss: 0.3748 | Val Loss: 0.5170 | Val F1: 0.4537 | Val P: 0.3660 | Val R: 0.5966\n",
      "2026-03-01 08:37:02,509 INFO:\tStep 1715 | Train Loss: 0.4898 | Val Loss: 0.5074 | Val F1: 0.4576 | Val P: 0.4079 | Val R: 0.5210\n",
      "2026-03-01 08:40:10,927 INFO:\tStep 1750 | Train Loss: 0.4996 | Val Loss: 0.5171 | Val F1: 0.4293 | Val P: 0.3118 | Val R: 0.6891\n",
      "2026-03-01 08:43:19,066 INFO:\tStep 1785 | Train Loss: 0.4473 | Val Loss: 0.5869 | Val F1: 0.4390 | Val P: 0.4252 | Val R: 0.4538\n",
      "2026-03-01 08:43:25,062 INFO:\tEpoch 9/12\n",
      "2026-03-01 08:46:23,335 INFO:\tStep 1820 | Train Loss: 0.4952 | Val Loss: 0.5385 | Val F1: 0.4421 | Val P: 0.3795 | Val R: 0.5294\n",
      "2026-03-01 08:49:31,650 INFO:\tStep 1855 | Train Loss: 0.4771 | Val Loss: 0.5765 | Val F1: 0.4533 | Val P: 0.4811 | Val R: 0.4286\n",
      "2026-03-01 08:52:40,199 INFO:\tStep 1890 | Train Loss: 0.4429 | Val Loss: 0.5477 | Val F1: 0.4464 | Val P: 0.3407 | Val R: 0.6471\n",
      "2026-03-01 08:55:43,641 INFO:\tStep 1925 | Train Loss: 0.4879 | Val Loss: 0.5908 | Val F1: 0.4352 | Val P: 0.4845 | Val R: 0.3950\n",
      "2026-03-01 08:58:29,461 INFO:\tStep 1960 | Train Loss: 0.4974 | Val Loss: 0.5875 | Val F1: 0.4259 | Val P: 0.4742 | Val R: 0.3866\n",
      "2026-03-01 09:01:19,560 INFO:\tStep 1995 | Train Loss: 0.4325 | Val Loss: 0.5924 | Val F1: 0.4324 | Val P: 0.4660 | Val R: 0.4034\n",
      "2026-03-01 09:02:12,170 INFO:\tEpoch 10/12\n",
      "2026-03-01 09:04:03,571 INFO:\tStep 2030 | Train Loss: 0.4721 | Val Loss: 0.5686 | Val F1: 0.4382 | Val P: 0.4167 | Val R: 0.4622\n",
      "2026-03-01 09:06:58,784 INFO:\tStep 2065 | Train Loss: 0.4070 | Val Loss: 0.5949 | Val F1: 0.4375 | Val P: 0.4667 | Val R: 0.4118\n",
      "2026-03-01 09:09:53,552 INFO:\tStep 2100 | Train Loss: 0.4825 | Val Loss: 0.5576 | Val F1: 0.4564 | Val P: 0.4508 | Val R: 0.4622\n",
      "2026-03-01 09:12:56,451 INFO:\tStep 2135 | Train Loss: 0.4986 | Val Loss: 0.5469 | Val F1: 0.4653 | Val P: 0.4524 | Val R: 0.4790\n",
      "2026-03-01 09:16:04,698 INFO:\tStep 2170 | Train Loss: 0.4109 | Val Loss: 0.5625 | Val F1: 0.4701 | Val P: 0.4783 | Val R: 0.4622\n",
      "2026-03-01 09:19:15,183 INFO:\tStep 2205 | Train Loss: 0.5079 | Val Loss: 0.6066 | Val F1: 0.4058 | Val P: 0.4773 | Val R: 0.3529\n",
      "2026-03-01 09:21:08,654 INFO:\tEpoch 11/12\n",
      "2026-03-01 09:22:21,267 INFO:\tStep 2240 | Train Loss: 0.4489 | Val Loss: 0.5999 | Val F1: 0.3918 | Val P: 0.5067 | Val R: 0.3193\n",
      "2026-03-01 09:25:30,693 INFO:\tStep 2275 | Train Loss: 0.4525 | Val Loss: 0.5719 | Val F1: 0.4292 | Val P: 0.4700 | Val R: 0.3950\n",
      "2026-03-01 09:28:40,175 INFO:\tStep 2310 | Train Loss: 0.4582 | Val Loss: 0.5917 | Val F1: 0.4115 | Val P: 0.4778 | Val R: 0.3613\n",
      "2026-03-01 09:31:54,144 INFO:\tStep 2345 | Train Loss: 0.5617 | Val Loss: 0.5932 | Val F1: 0.3834 | Val P: 0.5000 | Val R: 0.3109\n",
      "2026-03-01 09:35:06,643 INFO:\tStep 2380 | Train Loss: 0.5205 | Val Loss: 0.5684 | Val F1: 0.4058 | Val P: 0.4773 | Val R: 0.3529\n",
      "2026-03-01 09:38:19,144 INFO:\tStep 2415 | Train Loss: 0.4530 | Val Loss: 0.5564 | Val F1: 0.4190 | Val P: 0.4835 | Val R: 0.3697\n",
      "2026-03-01 09:41:31,554 INFO:\tStep 2450 | Train Loss: 0.4646 | Val Loss: 0.5661 | Val F1: 0.4038 | Val P: 0.4719 | Val R: 0.3529\n",
      "2026-03-01 09:41:59,381 INFO:\tEpoch 12/12\n",
      "2026-03-01 09:44:40,242 INFO:\tStep 2485 | Train Loss: 0.4730 | Val Loss: 0.5480 | Val F1: 0.4352 | Val P: 0.4845 | Val R: 0.3950\n",
      "2026-03-01 09:44:40,243 INFO:\tEarly stopping at step 2485\n",
      "2026-03-01 09:45:39,002 INFO:\tOptimal threshold: 0.500 (val F1 with threshold: 0.4786, val F1 @0.5: 0.4786)\n",
      "2026-03-01 09:47:03,603 INFO:\tDev Loss: 0.5273 | Dev F1: 0.4407 | Dev P: 0.4252 | Dev R: 0.4573 | Threshold: 0.500\n",
      "2026-03-01 09:47:22,753 INFO:\t[F_large] New best saved (val F1=0.4786)\n",
      "\u001b[32m[I 2026-03-01 09:47:25,649]\u001b[0m Trial 3 finished with value: 0.47863247863247865 and parameters: {'lr': 4.649345697900868e-06, 'hidden_dim': 256, 'dropout_rate': 0.1, 'weight_decay': 0.000747599299995651, 'head_lr_multiplier': 10}. Best is trial 3 with value: 0.47863247863247865.\u001b[0m\n",
      "2026-03-01 09:47:25,655 INFO:\t[F_large] Trial 4: lr=2.96e-06, pool=PoolingStrategy.CLS_MEAN, hidden=256, wd=2.4e-03, head_lr_mult=10\n",
      "2026-03-01 09:47:28,797 INFO:\tHTTP Request: HEAD https://huggingface.co/microsoft/deberta-v3-large/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-03-01 09:47:28,806 INFO:\tHTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/microsoft/deberta-v3-large/64a8c8eab3e352a784c658aef62be1662607476f/config.json \"HTTP/1.1 200 OK\"\n",
      "2026-03-01 09:47:28,905 INFO:\tHTTP Request: HEAD https://huggingface.co/microsoft/deberta-v3-large/resolve/main/model.safetensors \"HTTP/1.1 404 Not Found\"\n",
      "2026-03-01 09:47:29,005 INFO:\tHTTP Request: GET https://huggingface.co/api/models/microsoft/deberta-v3-large \"HTTP/1.1 200 OK\"\n",
      "2026-03-01 09:47:29,120 INFO:\tHTTP Request: GET https://huggingface.co/api/models/microsoft/deberta-v3-large/commits/main \"HTTP/1.1 200 OK\"\n",
      "2026-03-01 09:47:29,233 INFO:\tHTTP Request: GET https://huggingface.co/api/models/microsoft/deberta-v3-large/discussions?p=0 \"HTTP/1.1 200 OK\"\n",
      "2026-03-01 09:47:29,348 INFO:\tHTTP Request: GET https://huggingface.co/api/models/microsoft/deberta-v3-large/commits/refs%2Fpr%2F13 \"HTTP/1.1 200 OK\"\n",
      "2026-03-01 09:47:29,443 INFO:\tHTTP Request: HEAD https://huggingface.co/microsoft/deberta-v3-large/resolve/refs%2Fpr%2F13/model.safetensors.index.json \"HTTP/1.1 404 Not Found\"\n",
      "2026-03-01 09:47:29,545 INFO:\tHTTP Request: HEAD https://huggingface.co/microsoft/deberta-v3-large/resolve/refs%2Fpr%2F13/model.safetensors \"HTTP/1.1 302 Found\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4918eb3d80024c64a366d8fd78e58fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mDebertaV2Model LOAD REPORT\u001b[0m from: microsoft/deberta-v3-large\n",
      "Key                                     | Status     |  | \n",
      "----------------------------------------+------------+--+-\n",
      "mask_predictions.dense.weight           | UNEXPECTED |  | \n",
      "mask_predictions.dense.bias             | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED |  | \n",
      "mask_predictions.classifier.weight      | UNEXPECTED |  | \n",
      "mask_predictions.LayerNorm.weight       | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED |  | \n",
      "mask_predictions.LayerNorm.bias         | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED |  | \n",
      "mask_predictions.classifier.bias        | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "2026-03-01 09:47:35,733 INFO:\tBackbone model loaded: DebertaV2Model, dtype torch.float32, gradient_checkpointing=False\n",
      "2026-03-01 09:47:36,362 INFO:\tpos_weight = 3.09 (raw ratio=9.55, neg=6443, pos=675)\n",
      "2026-03-01 09:47:36,367 INFO:\tUsing bitsandbytes 8-bit AdamW\n",
      "2026-03-01 09:47:36,368 INFO:\tEarly stopping: patience=4 epochs = 24 eval rounds (6 evals/epoch, eval every 35 steps)\n",
      "2026-03-01 09:47:36,369 INFO:\tGradient accumulation: 16 micro-steps per optimizer step\n",
      "2026-03-01 09:47:36,372 INFO:\tEpoch 1/12\n",
      "2026-03-01 09:50:44,725 INFO:\tStep 35 | Train Loss: 0.8350 | Val Loss: 0.7803 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 09:53:53,414 INFO:\tStep 70 | Train Loss: 0.7622 | Val Loss: 0.6866 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 09:57:01,878 INFO:\tStep 105 | Train Loss: 0.6506 | Val Loss: 0.6770 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 10:00:10,416 INFO:\tStep 140 | Train Loss: 0.6891 | Val Loss: 0.6666 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 10:03:18,769 INFO:\tStep 175 | Train Loss: 0.6678 | Val Loss: 0.6752 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 10:06:27,480 INFO:\tStep 210 | Train Loss: 0.6804 | Val Loss: 0.6657 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 10:07:17,053 INFO:\tEpoch 2/12\n",
      "2026-03-01 10:09:32,153 INFO:\tStep 245 | Train Loss: 0.6441 | Val Loss: 0.6691 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 10:12:40,489 INFO:\tStep 280 | Train Loss: 0.6581 | Val Loss: 0.6653 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 10:15:48,726 INFO:\tStep 315 | Train Loss: 0.6945 | Val Loss: 0.6758 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 10:18:57,166 INFO:\tStep 350 | Train Loss: 0.6771 | Val Loss: 0.6657 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 10:22:05,523 INFO:\tStep 385 | Train Loss: 0.7267 | Val Loss: 0.6640 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 10:25:13,839 INFO:\tStep 420 | Train Loss: 0.6629 | Val Loss: 0.6603 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 10:26:56,443 INFO:\tEpoch 3/12\n",
      "2026-03-01 10:28:18,376 INFO:\tStep 455 | Train Loss: 0.6299 | Val Loss: 0.6781 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 10:31:27,097 INFO:\tStep 490 | Train Loss: 0.5996 | Val Loss: 0.6673 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 10:34:35,526 INFO:\tStep 525 | Train Loss: 0.6785 | Val Loss: 0.6957 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 10:37:43,914 INFO:\tStep 560 | Train Loss: 0.6627 | Val Loss: 0.6471 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 10:40:49,499 INFO:\tStep 595 | Train Loss: 0.6580 | Val Loss: 0.7000 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "\u001b[33m[W 2026-03-01 10:43:06,788]\u001b[0m Trial 4 failed with parameters: {'lr': 2.9617890239950387e-06, 'hidden_dim': 256, 'dropout_rate': 0.3, 'weight_decay': 0.0023628864184236428, 'head_lr_multiplier': 10} because of the following error: KeyboardInterrupt().\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jlc/anaconda3/envs/mlenv/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 206, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_773276/2228136022.py\", line 38, in objective\n",
      "    results = train_model(\n",
      "  File \"/home/jlc/Year3/NLP/CW/exp/../utils/training_loop.py\", line 129, in train_model\n",
      "    val_metrics = evaluate(model, device, val_loader, criterion=criterion)\n",
      "  File \"/home/jlc/anaconda3/envs/mlenv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 124, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/jlc/Year3/NLP/CW/exp/../utils/eval.py\", line 37, in evaluate\n",
      "    total_loss += criterion(unnormalised_scores, labels).item()\n",
      "  File \"/home/jlc/anaconda3/envs/mlenv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1776, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/jlc/anaconda3/envs/mlenv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1787, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/jlc/anaconda3/envs/mlenv/lib/python3.10/site-packages/torch/nn/modules/loss.py\", line 833, in forward\n",
      "    return F.binary_cross_entropy_with_logits(\n",
      "  File \"/home/jlc/anaconda3/envs/mlenv/lib/python3.10/site-packages/torch/nn/functional.py\", line 3641, in binary_cross_entropy_with_logits\n",
      "    return torch.binary_cross_entropy_with_logits(\n",
      "KeyboardInterrupt\n",
      "\u001b[33m[W 2026-03-01 10:43:06,792]\u001b[0m Trial 4 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m      4\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(\n\u001b[1;32m      5\u001b[0m     direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     study_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpcl_deberta_exp_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEXP_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     sampler\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTPESampler(seed\u001b[38;5;241m=\u001b[39mSEED),\n\u001b[1;32m      8\u001b[0m     pruner\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39mpruners\u001b[38;5;241m.\u001b[39mMedianPruner(n_startup_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, n_warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m),\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_TRIALS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m best \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\n\u001b[1;32m     13\u001b[0m LOG\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest\u001b[38;5;241m.\u001b[39mnumber\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.10/site-packages/optuna/study/study.py:490\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    390\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    398\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    399\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 490\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.10/site-packages/optuna/study/_optimize.py:68\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 68\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.10/site-packages/optuna/study/_optimize.py:165\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     frozen_trial_id \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.10/site-packages/optuna/study/_optimize.py:263\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    259\u001b[0m     updated_state \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    262\u001b[0m ):\n\u001b[0;32m--> 263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trial\u001b[38;5;241m.\u001b[39m_trial_id\n",
      "File \u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.10/site-packages/optuna/study/_optimize.py:206\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 206\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[3], line 38\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     28\u001b[0m model \u001b[38;5;241m=\u001b[39m PCLDeBERTa(\n\u001b[1;32m     29\u001b[0m     hidden_dim\u001b[38;5;241m=\u001b[39mhidden_dim,\n\u001b[1;32m     30\u001b[0m     dropout_rate\u001b[38;5;241m=\u001b[39mdropout_rate,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     gradient_checkpointing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# incompatible with DeBERTa-v3 + grad accumulation\u001b[39;00m\n\u001b[1;32m     34\u001b[0m )\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     36\u001b[0m pos_weight \u001b[38;5;241m=\u001b[39m compute_pos_weight(train_sub_df, DEVICE)\n\u001b[0;32m---> 38\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdev_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup_fraction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mWARMUP_FRACTION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPATIENCE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_lr_multiplier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_lr_mult\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLABEL_SMOOTHING\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_every_n_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_EVAL_STEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mACCUMULATION_STEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_8bit_adam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUSE_8BIT_ADAM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m trial\u001b[38;5;241m.\u001b[39mset_user_attr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_val_f1\u001b[39m\u001b[38;5;124m\"\u001b[39m,    results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_val_f1\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     52\u001b[0m trial\u001b[38;5;241m.\u001b[39mset_user_attr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m, results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/Year3/NLP/CW/exp/../utils/training_loop.py:129\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, device, train_loader, val_loader, dev_loader, pos_weight, lr, weight_decay, num_epochs, warmup_fraction, patience, head_lr_multiplier, label_smoothing, eval_every_n_steps, accumulate_grad_batches, use_8bit_adam, trial)\u001b[0m\n\u001b[1;32m    126\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    127\u001b[0m running_micro_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 129\u001b[0m val_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m val_f1 \u001b[38;5;241m=\u001b[39m val_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    131\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m val_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.10/site-packages/torch/utils/_contextlib.py:124\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 124\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Year3/NLP/CW/exp/../utils/eval.py:37\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, device, dataloader, criterion, threshold)\u001b[0m\n\u001b[1;32m     35\u001b[0m     all_scores\u001b[38;5;241m.\u001b[39mappend(unnormalised_scores\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[1;32m     36\u001b[0m     all_labels\u001b[38;5;241m.\u001b[39mappend(labels\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[0;32m---> 37\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43munnormalised_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     38\u001b[0m     num_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     40\u001b[0m all_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(all_scores)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1776\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1787\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1786\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1789\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1790\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.10/site-packages/torch/nn/modules/loss.py:833\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    832\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs the forward pass.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 833\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.10/site-packages/torch/nn/functional.py:3641\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[1;32m   3637\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3638\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3639\u001b[0m     )\n\u001b[0;32m-> 3641\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3642\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\n\u001b[1;32m   3643\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=f\"pcl_deberta_exp_{EXP_NAME}\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=SEED),\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=4, n_warmup_steps=200),\n",
    ")\n",
    "study.optimize(objective, n_trials=N_TRIALS)\n",
    "\n",
    "best = study.best_trial\n",
    "LOG.info(f\"Best trial: {best.number}\")\n",
    "LOG.info(f\"Val F1: {best.user_attrs['best_val_f1']:.4f} | Dev F1: {best.user_attrs['dev_f1']:.4f}\")\n",
    "LOG.info(f\"Best params: {best.params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0000009",
   "metadata": {},
   "source": [
    "## 4. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0000010",
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot_fn, suffix in [\n",
    "    (plot_optimization_history, \"history\"),\n",
    "    (plot_param_importances, \"importances\"),\n",
    "    (plot_parallel_coordinate, \"parallel\"),\n",
    "]:\n",
    "    plot_fn(study)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUT_DIR}/{EXP_NAME}_optuna_{suffix}.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "best = study.best_trial\n",
    "best_params = best.params\n",
    "pooling = POOLING_MAP[best_params[\"pooling\"]]\n",
    "\n",
    "model = PCLDeBERTa(\n",
    "    hidden_dim=best_params[\"hidden_dim\"],\n",
    "    dropout_rate=best_params.get(\"dropout_rate\", 0.0),\n",
    "    pooling=pooling,\n",
    "    model_name=MODEL_NAME,\n",
    ").to(DEVICE)\n",
    "\n",
    "state_dict = torch.load(\n",
    "    os.path.join(OUT_DIR, f\"exp_{EXP_NAME}_best_model.pt\"), map_location=DEVICE\n",
    ")\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "_, _, dev_loader = make_dataloaders(\n",
    "    train_sub_df, val_sub_df, dev_df, MICRO_BATCH_SIZE, MAX_LENGTH, tokeniser\n",
    ")\n",
    "dev_metrics = evaluate(model, DEVICE, dev_loader, threshold=best.user_attrs[\"best_threshold\"])\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"{EXP_NAME.upper()} — Dev Set Results (threshold={best.user_attrs['best_threshold']:.3f})\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Effective batch: {MICRO_BATCH_SIZE} × {ACCUMULATION_STEPS} = {MICRO_BATCH_SIZE*ACCUMULATION_STEPS}\")\n",
    "print(f\"8-bit AdamW: {USE_8BIT_ADAM}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(classification_report(dev_metrics[\"labels\"], dev_metrics[\"preds\"], target_names=[\"Non-PCL\", \"PCL\"]))\n",
    "print(\"Best hyperparams:\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2e62e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cd0c57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79d5446",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
