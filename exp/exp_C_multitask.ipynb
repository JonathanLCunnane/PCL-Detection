{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0000001",
   "metadata": {},
   "source": [
    "# Experiment C: Multi-task PCL + Per-Category Classification\n",
    "\n",
    "Train `PCLCategoryDeBERTa` with a **single unified head** (`1 + n_categories` logits):\n",
    "- **logits[:, 0]**: overall PCL vs non-PCL — binary BCE + `pos_weight`\n",
    "- **logits[:, 1:]**: multi-label per PCL category — BCE (one logit per category)\n",
    "\n",
    "A single `PCLClassifierHead(n_out=1+n_categories)` produces all outputs from the\n",
    "same shared pooled representation. Category labels come from\n",
    "`dontpatronizeme_categories.tsv`: a multi-hot vector indicating which of the 7\n",
    "PCL categories appear in each paragraph.  \n",
    "Non-PCL paragraphs have all-zero category labels (confirmed: all 993 annotated\n",
    "paragraphs have binary_label=1).\n",
    "\n",
    "**PCL categories** (7):\n",
    "`Authority_voice`, `Compassion`, `Metaphors`, `Presupposition`,\n",
    "`Shallow_solution`, `The_poorer_the_merrier`, `Unbalanced_power_relations`\n",
    "\n",
    "Total loss = `binary_BCE + category_weight × category_BCE`  \n",
    "A single `BCEWithLogitsLoss(reduction='none')` is applied to the combined\n",
    "`(B, 1+n_categories)` output; the binary and category components are then\n",
    "averaged separately before weighting.\n",
    "\n",
    "Fixed hyperparameters (not searched):\n",
    "- VAL_FRACTION=0.15, BATCH_SIZE=32, NUM_EPOCHS=12, PATIENCE=4\n",
    "- `pooling=MEAN` — best default for DeBERTa-v3 (RTD pretraining; no special CLS)\n",
    "- `warmup_fraction=0.10`, `label_smoothing=0.0` — fixed to save trials for key params\n",
    "\n",
    "Searched: `lr`, `weight_decay`, `hidden_dim ∈ {0, 256}`, `dropout_rate ∈ {0.1, 0.3}`,\n",
    "`head_lr_multiplier ∈ {1, 3, 5}`, `category_weight ∈ {0.1, 0.2, 0.5, 1.0}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0000002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-03-01 11:37:05,608 INFO:\tDevice: cuda\n",
      "2026-03-01 11:37:05,609 INFO:\tPCL categories (7): ['Authority_voice', 'Compassion', 'Metaphors', 'Presupposition', 'Shallow_solution', 'The_poorer_the_merrier', 'Unbalanced_power_relations']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import logging\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.metrics import classification_report\n",
    "import optuna\n",
    "from optuna.visualization.matplotlib import (\n",
    "    plot_optimization_history,\n",
    "    plot_param_importances,\n",
    "    plot_parallel_coordinate,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "from utils.data import load_data_categories, PCL_CATEGORIES\n",
    "from utils.split import split_train_val\n",
    "from utils.dataloaders import make_category_dataloaders\n",
    "from utils.pcl_deberta import PCLDeBERTa, PoolingStrategy\n",
    "from utils.optim import compute_pos_weight\n",
    "from utils.training_loop import train_category_model\n",
    "\n",
    "SEED = 42\n",
    "DATA_DIR = \"../data\"\n",
    "OUT_DIR = \"out\"\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "MAX_LENGTH = 256\n",
    "VAL_FRACTION = 0.15\n",
    "BATCH_SIZE = 32\n",
    "N_TRIALS = 20\n",
    "NUM_EPOCHS = 12\n",
    "PATIENCE = 4\n",
    "N_EVAL_STEPS = 35\n",
    "N_CATEGORIES = len(PCL_CATEGORIES)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s:\\t%(message)s\")\n",
    "LOG = logging.getLogger(__name__)\n",
    "LOG.info(f\"Device: {DEVICE}\")\n",
    "LOG.info(f\"PCL categories ({N_CATEGORIES}): {PCL_CATEGORIES}\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0000003",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "`load_data_categories` returns DataFrames with columns:\n",
    "`text`, `binary_label` (0/1), and one binary column per `PCL_CATEGORIES` entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0000004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-03-01 11:37:05,915 INFO:\tTrain/val split: 7118 train, 1257 val (val_frac=0.15)\n",
      "2026-03-01 11:37:05,918 INFO:\tTrain, val positive count: 675, 119\n",
      "2026-03-01 11:37:06,044 INFO:\tHTTP Request: HEAD https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-03-01 11:37:06,052 INFO:\tHTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/microsoft/deberta-v3-base/8ccc9b6f36199bec6961081d44eb72fb3f7353f3/config.json \"HTTP/1.1 200 OK\"\n",
      "2026-03-01 11:37:06,148 INFO:\tHTTP Request: HEAD https://huggingface.co/microsoft/deberta-v3-base/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-03-01 11:37:06,156 INFO:\tHTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/microsoft/deberta-v3-base/8ccc9b6f36199bec6961081d44eb72fb3f7353f3/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
      "2026-03-01 11:37:06,253 INFO:\tHTTP Request: GET https://huggingface.co/api/models/microsoft/deberta-v3-base/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
      "2026-03-01 11:37:06,355 INFO:\tHTTP Request: GET https://huggingface.co/api/models/microsoft/deberta-v3-base/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
      "2026-03-01 11:37:06,869 INFO:\tHTTP Request: GET https://huggingface.co/api/models/microsoft/deberta-v3-base \"HTTP/1.1 200 OK\"\n",
      "2026-03-01 11:37:06,871 INFO:\tTrain: 7118, Val: 1257, Dev: 2093\n",
      "2026-03-01 11:37:06,872 INFO:\tCategory label counts in train_sub:\n",
      "2026-03-01 11:37:06,873 INFO:\t  Authority_voice: 162 (2.3%)\n",
      "2026-03-01 11:37:06,875 INFO:\t  Compassion: 309 (4.3%)\n",
      "2026-03-01 11:37:06,876 INFO:\t  Metaphors: 125 (1.8%)\n",
      "2026-03-01 11:37:06,877 INFO:\t  Presupposition: 133 (1.9%)\n",
      "2026-03-01 11:37:06,879 INFO:\t  Shallow_solution: 134 (1.9%)\n",
      "2026-03-01 11:37:06,881 INFO:\t  The_poorer_the_merrier: 25 (0.4%)\n",
      "2026-03-01 11:37:06,882 INFO:\t  Unbalanced_power_relations: 487 (6.8%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>binary_label</th>\n",
       "      <th>Authority_voice</th>\n",
       "      <th>Compassion</th>\n",
       "      <th>Metaphors</th>\n",
       "      <th>Presupposition</th>\n",
       "      <th>Shallow_solution</th>\n",
       "      <th>The_poorer_the_merrier</th>\n",
       "      <th>Unbalanced_power_relations</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>par_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Arshad said that besides learning many new asp...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Fast food employee who fed disabled man become...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Vanessa had feelings of hopelessness in her fi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>In September , Major Nottle set off on foot fr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>The demographics of Pakistan and India are ver...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  binary_label  \\\n",
       "par_id                                                                    \n",
       "33      Arshad said that besides learning many new asp...             1   \n",
       "34      Fast food employee who fed disabled man become...             1   \n",
       "42      Vanessa had feelings of hopelessness in her fi...             1   \n",
       "77      In September , Major Nottle set off on foot fr...             1   \n",
       "83      The demographics of Pakistan and India are ver...             1   \n",
       "\n",
       "        Authority_voice  Compassion  Metaphors  Presupposition  \\\n",
       "par_id                                                           \n",
       "33                    0           0          0               0   \n",
       "34                    0           0          0               0   \n",
       "42                    0           1          0               0   \n",
       "77                    0           0          0               0   \n",
       "83                    0           0          0               0   \n",
       "\n",
       "        Shallow_solution  The_poorer_the_merrier  Unbalanced_power_relations  \n",
       "par_id                                                                        \n",
       "33                     0                       0                           1  \n",
       "34                     1                       0                           1  \n",
       "42                     0                       0                           0  \n",
       "77                     1                       0                           1  \n",
       "83                     1                       0                           1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, dev_df = load_data_categories(DATA_DIR)\n",
    "train_sub_df, val_sub_df = split_train_val(train_df, val_frac=VAL_FRACTION, seed=SEED)\n",
    "tokeniser = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "LOG.info(f\"Train: {len(train_sub_df)}, Val: {len(val_sub_df)}, Dev: {len(dev_df)}\")\n",
    "LOG.info(\"Category label counts in train_sub:\")\n",
    "for cat in PCL_CATEGORIES:\n",
    "    n = train_sub_df[cat].sum()\n",
    "    LOG.info(f\"  {cat}: {n} ({n/len(train_sub_df)*100:.1f}%)\")\n",
    "\n",
    "train_df[train_df[\"binary_label\"] == 1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0000005",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter Search\n",
    "\n",
    "`category_weight` is the key experiment-specific hyperparameter, controlling how\n",
    "much the per-category auxiliary loss influences the shared representation.\n",
    "\n",
    "Secondary hyperparameters are narrowed to avoid wasting trials:\n",
    "- `pooling` fixed to MEAN (DeBERTa-v3 RTD pretraining; no special CLS token)\n",
    "- `warmup_fraction` fixed to 0.10, `label_smoothing` fixed to 0.0\n",
    "- `hidden_dim ∈ {0, 256}` (0 = single linear, 256 = MLP)\n",
    "- `dropout_rate ∈ {0.1, 0.3}` (only sampled when hidden_dim=256)\n",
    "- `head_lr_multiplier ∈ {1, 3, 5}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0000006",
   "metadata": {},
   "outputs": [],
   "source": [
    "POOLING = PoolingStrategy.CLS_MEAN   # fixed: best default for DeBERTa-v3 RTD pretraining\n",
    "EXP_NAME = \"C_multitask\"\n",
    "\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    lr              = trial.suggest_float(\"lr\", 4e-6, 6e-5, log=True)\n",
    "    weight_decay    = trial.suggest_float(\"weight_decay\", 1e-5, 1e-2, log=True)\n",
    "    hidden_dim      = trial.suggest_categorical(\"hidden_dim\", [0, 256])\n",
    "    dropout_rate    = trial.suggest_categorical(\"dropout_rate\", [0.1, 0.3]) if hidden_dim > 0 else 0.0\n",
    "    head_lr_mult    = trial.suggest_categorical(\"head_lr_multiplier\", [1, 3, 5])\n",
    "    category_weight = trial.suggest_categorical(\"category_weight\", [0.1, 0.2, 0.5, 1.0])\n",
    "\n",
    "    # Fixed — not worth spending trials on\n",
    "    warmup_fraction = 0.10\n",
    "    label_smoothing = 0.0\n",
    "\n",
    "    LOG.info(f\"[{EXP_NAME}] Trial {trial.number}: lr={lr:.2e}, hidden={hidden_dim}, \"\n",
    "             f\"cat_w={category_weight}, head_lr_mult={head_lr_mult}\")\n",
    "\n",
    "    train_loader, val_loader, dev_loader = make_category_dataloaders(\n",
    "        train_sub_df, val_sub_df, dev_df, BATCH_SIZE, MAX_LENGTH, tokeniser\n",
    "    )\n",
    "\n",
    "    model = PCLDeBERTa(\n",
    "        hidden_dim=hidden_dim,\n",
    "        dropout_rate=dropout_rate,\n",
    "        pooling=POOLING,\n",
    "        n_out=1 + N_CATEGORIES,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    pos_weight = compute_pos_weight(train_sub_df, DEVICE)\n",
    "\n",
    "    results = train_category_model(\n",
    "        model=model, device=DEVICE,\n",
    "        train_loader=train_loader, val_loader=val_loader, dev_loader=dev_loader,\n",
    "        pos_weight=pos_weight, lr=lr, weight_decay=weight_decay,\n",
    "        num_epochs=NUM_EPOCHS, warmup_fraction=warmup_fraction,\n",
    "        patience=PATIENCE, category_weight=category_weight,\n",
    "        head_lr_multiplier=head_lr_mult,\n",
    "        label_smoothing=label_smoothing,\n",
    "        eval_every_n_steps=N_EVAL_STEPS,\n",
    "        trial=trial,\n",
    "    )\n",
    "\n",
    "    trial.set_user_attr(\"best_val_f1\",    results[\"best_val_f1\"])\n",
    "    trial.set_user_attr(\"best_threshold\", results[\"best_threshold\"])\n",
    "    trial.set_user_attr(\"dev_f1\",         results[\"dev_metrics\"][\"f1\"])\n",
    "    trial.set_user_attr(\"dev_precision\",  results[\"dev_metrics\"][\"precision\"])\n",
    "    trial.set_user_attr(\"dev_recall\",     results[\"dev_metrics\"][\"recall\"])\n",
    "\n",
    "    try:\n",
    "        prev_best = trial.study.best_value\n",
    "    except ValueError:\n",
    "        prev_best = -float(\"inf\")\n",
    "    if results[\"best_val_f1\"] > prev_best:\n",
    "        torch.save(\n",
    "            {k: v.cpu() for k, v in model.state_dict().items()},\n",
    "            os.path.join(OUT_DIR, f\"exp_{EXP_NAME}_best_model.pt\")\n",
    "        )\n",
    "        config = {\n",
    "            **trial.params,\n",
    "            \"pooling\": POOLING.name,\n",
    "            \"warmup_fraction\": warmup_fraction,\n",
    "            \"label_smoothing\": label_smoothing,\n",
    "            \"batch_size\": BATCH_SIZE, \"num_epochs\": NUM_EPOCHS, \"patience\": PATIENCE,\n",
    "            \"best_threshold\": results[\"best_threshold\"],\n",
    "            \"n_categories\": N_CATEGORIES, \"pcl_categories\": PCL_CATEGORIES,\n",
    "        }\n",
    "        with open(os.path.join(OUT_DIR, f\"exp_{EXP_NAME}_best_params.json\"), \"w\") as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        LOG.info(f\"[{EXP_NAME}] New best saved (val F1={results['best_val_f1']:.4f})\")\n",
    "\n",
    "    del model, train_loader, val_loader, dev_loader\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return results[\"best_val_f1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0000007",
   "metadata": {},
   "source": [
    "## 3. Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0000008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-03-01 11:37:07,343]\u001b[0m A new study created in memory with name: pcl_deberta_exp_C_multitask\u001b[0m\n",
      "2026-03-01 11:37:07,345 INFO:\t[C_multitask] Trial 0: lr=1.10e-05, hidden=0, cat_w=0.1, head_lr_mult=1\n",
      "2026-03-01 11:37:11,236 INFO:\tHTTP Request: HEAD https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-03-01 11:37:11,245 INFO:\tHTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/microsoft/deberta-v3-base/8ccc9b6f36199bec6961081d44eb72fb3f7353f3/config.json \"HTTP/1.1 200 OK\"\n",
      "2026-03-01 11:37:11,549 INFO:\tHTTP Request: HEAD https://huggingface.co/microsoft/deberta-v3-base/resolve/main/model.safetensors \"HTTP/1.1 404 Not Found\"\n",
      "2026-03-01 11:37:11,647 INFO:\tHTTP Request: GET https://huggingface.co/api/models/microsoft/deberta-v3-base \"HTTP/1.1 200 OK\"\n",
      "2026-03-01 11:37:11,756 INFO:\tHTTP Request: GET https://huggingface.co/api/models/microsoft/deberta-v3-base/commits/main \"HTTP/1.1 200 OK\"\n",
      "2026-03-01 11:37:11,871 INFO:\tHTTP Request: GET https://huggingface.co/api/models/microsoft/deberta-v3-base/discussions?p=0 \"HTTP/1.1 200 OK\"\n",
      "2026-03-01 11:37:11,988 INFO:\tHTTP Request: GET https://huggingface.co/api/models/microsoft/deberta-v3-base/commits/refs%2Fpr%2F14 \"HTTP/1.1 200 OK\"\n",
      "2026-03-01 11:37:12,088 INFO:\tHTTP Request: HEAD https://huggingface.co/microsoft/deberta-v3-base/resolve/refs%2Fpr%2F14/model.safetensors.index.json \"HTTP/1.1 404 Not Found\"\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "2026-03-01 11:37:12,091 WARNING:\tWarning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d7869561bb4b138afc724da5aff8b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-03-01 11:37:12,201 INFO:\tHTTP Request: HEAD https://huggingface.co/microsoft/deberta-v3-base/resolve/refs%2Fpr%2F14/model.safetensors \"HTTP/1.1 302 Found\"\n",
      "\u001b[1mDebertaV2Model LOAD REPORT\u001b[0m from: microsoft/deberta-v3-base\n",
      "Key                                     | Status     |  | \n",
      "----------------------------------------+------------+--+-\n",
      "lm_predictions.lm_head.bias             | UNEXPECTED |  | \n",
      "mask_predictions.LayerNorm.bias         | UNEXPECTED |  | \n",
      "mask_predictions.LayerNorm.weight       | UNEXPECTED |  | \n",
      "mask_predictions.classifier.bias        | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED |  | \n",
      "mask_predictions.dense.bias             | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED |  | \n",
      "mask_predictions.dense.weight           | UNEXPECTED |  | \n",
      "mask_predictions.classifier.weight      | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "2026-03-01 11:37:12,834 INFO:\tBackbone model loaded: DebertaV2Model, dtype torch.float32, gradient_checkpointing=True\n",
      "2026-03-01 11:37:13,477 INFO:\tpos_weight = 3.09 (raw ratio=9.55, neg=6443, pos=675)\n",
      "2026-03-01 11:37:13,499 INFO:\t[CategoryModel] Early stopping: patience=4 epochs = 24 eval rounds\n",
      "2026-03-01 11:37:13,502 INFO:\t[CategoryModel] Epoch 1/12\n",
      "2026-03-01 11:38:13,089 INFO:\tStep 35 | Train Loss: 0.8413 | Val Loss: 0.6831 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 11:39:12,290 INFO:\tStep 70 | Train Loss: 0.7380 | Val Loss: 0.6470 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 11:40:09,983 INFO:\tStep 105 | Train Loss: 0.7035 | Val Loss: 0.6291 | Val F1: 0.0161 | Val P: 0.2000 | Val R: 0.0084\n",
      "2026-03-01 11:41:07,593 INFO:\tStep 140 | Train Loss: 0.6138 | Val Loss: 0.5914 | Val F1: 0.3151 | Val P: 0.2552 | Val R: 0.4118\n",
      "2026-03-01 11:42:06,751 INFO:\tStep 175 | Train Loss: 0.6169 | Val Loss: 0.5271 | Val F1: 0.3863 | Val P: 0.3069 | Val R: 0.5210\n",
      "2026-03-01 11:43:04,356 INFO:\tStep 210 | Train Loss: 0.6091 | Val Loss: 0.4847 | Val F1: 0.4080 | Val P: 0.3893 | Val R: 0.4286\n",
      "2026-03-01 11:43:18,977 INFO:\t[CategoryModel] Epoch 2/12\n",
      "2026-03-01 11:43:55,251 INFO:\tStep 245 | Train Loss: 0.5890 | Val Loss: 0.4757 | Val F1: 0.4691 | Val P: 0.3707 | Val R: 0.6387\n",
      "2026-03-01 11:44:52,541 INFO:\tStep 280 | Train Loss: 0.5675 | Val Loss: 0.4593 | Val F1: 0.4556 | Val P: 0.3402 | Val R: 0.6891\n",
      "2026-03-01 11:45:49,276 INFO:\tStep 315 | Train Loss: 0.5447 | Val Loss: 0.4724 | Val F1: 0.4484 | Val P: 0.4808 | Val R: 0.4202\n",
      "2026-03-01 11:46:41,497 INFO:\tStep 350 | Train Loss: 0.5800 | Val Loss: 0.6194 | Val F1: 0.3490 | Val P: 0.2180 | Val R: 0.8739\n",
      "2026-03-01 11:47:32,722 INFO:\tStep 385 | Train Loss: 0.5103 | Val Loss: 0.4897 | Val F1: 0.4566 | Val P: 0.3239 | Val R: 0.7731\n",
      "2026-03-01 11:48:23,468 INFO:\tStep 420 | Train Loss: 0.5177 | Val Loss: 0.4626 | Val F1: 0.4819 | Val P: 0.3483 | Val R: 0.7815\n",
      "2026-03-01 11:48:52,620 INFO:\t[CategoryModel] Epoch 3/12\n",
      "2026-03-01 11:49:13,751 INFO:\tStep 455 | Train Loss: 0.5116 | Val Loss: 0.4680 | Val F1: 0.4585 | Val P: 0.3230 | Val R: 0.7899\n",
      "2026-03-01 11:50:04,025 INFO:\tStep 490 | Train Loss: 0.4837 | Val Loss: 0.4723 | Val F1: 0.4306 | Val P: 0.2971 | Val R: 0.7815\n",
      "2026-03-01 11:50:54,851 INFO:\tStep 525 | Train Loss: 0.4329 | Val Loss: 0.4375 | Val F1: 0.5271 | Val P: 0.4620 | Val R: 0.6134\n",
      "2026-03-01 11:51:47,351 INFO:\tStep 560 | Train Loss: 0.4496 | Val Loss: 0.4269 | Val F1: 0.5343 | Val P: 0.4684 | Val R: 0.6218\n",
      "2026-03-01 11:52:38,711 INFO:\tStep 595 | Train Loss: 0.4332 | Val Loss: 0.5843 | Val F1: 0.4111 | Val P: 0.2759 | Val R: 0.8067\n",
      "2026-03-01 11:53:29,160 INFO:\tStep 630 | Train Loss: 0.4526 | Val Loss: 0.4231 | Val F1: 0.5217 | Val P: 0.4333 | Val R: 0.6555\n",
      "2026-03-01 11:54:19,678 INFO:\tStep 665 | Train Loss: 0.3878 | Val Loss: 0.4771 | Val F1: 0.5274 | Val P: 0.4451 | Val R: 0.6471\n",
      "2026-03-01 11:54:24,617 INFO:\t[CategoryModel] Epoch 4/12\n",
      "2026-03-01 11:55:17,540 INFO:\tStep 700 | Train Loss: 0.3607 | Val Loss: 0.5121 | Val F1: 0.4793 | Val P: 0.5306 | Val R: 0.4370\n",
      "2026-03-01 11:56:14,961 INFO:\tStep 735 | Train Loss: 0.4099 | Val Loss: 0.4629 | Val F1: 0.4548 | Val P: 0.3207 | Val R: 0.7815\n",
      "2026-03-01 11:57:12,327 INFO:\tStep 770 | Train Loss: 0.3148 | Val Loss: 0.6345 | Val F1: 0.5046 | Val P: 0.5556 | Val R: 0.4622\n",
      "2026-03-01 11:58:09,798 INFO:\tStep 805 | Train Loss: 0.3514 | Val Loss: 0.4794 | Val F1: 0.5028 | Val P: 0.3787 | Val R: 0.7479\n",
      "2026-03-01 11:59:07,229 INFO:\tStep 840 | Train Loss: 0.3993 | Val Loss: 0.5280 | Val F1: 0.4077 | Val P: 0.2738 | Val R: 0.7983\n",
      "2026-03-01 12:00:05,512 INFO:\tStep 875 | Train Loss: 0.3892 | Val Loss: 0.5126 | Val F1: 0.4289 | Val P: 0.2932 | Val R: 0.7983\n",
      "2026-03-01 12:00:28,558 INFO:\t[CategoryModel] Epoch 5/12\n",
      "2026-03-01 12:01:03,972 INFO:\tStep 910 | Train Loss: 0.4131 | Val Loss: 0.5279 | Val F1: 0.4840 | Val P: 0.3541 | Val R: 0.7647\n",
      "2026-03-01 12:02:02,608 INFO:\tStep 945 | Train Loss: 0.3125 | Val Loss: 0.7297 | Val F1: 0.4569 | Val P: 0.5769 | Val R: 0.3782\n",
      "2026-03-01 12:03:00,366 INFO:\tStep 980 | Train Loss: 0.3409 | Val Loss: 0.4917 | Val F1: 0.4934 | Val P: 0.3605 | Val R: 0.7815\n",
      "2026-03-01 12:03:58,142 INFO:\tStep 1015 | Train Loss: 0.2826 | Val Loss: 0.6059 | Val F1: 0.5197 | Val P: 0.4270 | Val R: 0.6639\n",
      "2026-03-01 12:04:56,790 INFO:\tStep 1050 | Train Loss: 0.3001 | Val Loss: 0.4958 | Val F1: 0.5130 | Val P: 0.3904 | Val R: 0.7479\n",
      "2026-03-01 12:05:53,267 INFO:\tStep 1085 | Train Loss: 0.3266 | Val Loss: 0.5080 | Val F1: 0.5333 | Val P: 0.4171 | Val R: 0.7395\n",
      "2026-03-01 12:06:30,415 INFO:\t[CategoryModel] Epoch 6/12\n",
      "2026-03-01 12:06:49,168 INFO:\tStep 1120 | Train Loss: 0.3421 | Val Loss: 0.6840 | Val F1: 0.4689 | Val P: 0.5444 | Val R: 0.4118\n",
      "2026-03-01 12:07:45,829 INFO:\tStep 1155 | Train Loss: 0.2021 | Val Loss: 0.6661 | Val F1: 0.5296 | Val P: 0.5000 | Val R: 0.5630\n",
      "2026-03-01 12:08:45,958 INFO:\tStep 1190 | Train Loss: 0.2836 | Val Loss: 0.5868 | Val F1: 0.5484 | Val P: 0.4450 | Val R: 0.7143\n",
      "2026-03-01 12:09:46,878 INFO:\tStep 1225 | Train Loss: 0.2791 | Val Loss: 0.7069 | Val F1: 0.4932 | Val P: 0.3659 | Val R: 0.7563\n",
      "2026-03-01 12:10:47,181 INFO:\tStep 1260 | Train Loss: 0.2566 | Val Loss: 0.6339 | Val F1: 0.5298 | Val P: 0.4372 | Val R: 0.6723\n",
      "2026-03-01 12:11:47,625 INFO:\tStep 1295 | Train Loss: 0.2578 | Val Loss: 0.5253 | Val F1: 0.5027 | Val P: 0.3725 | Val R: 0.7731\n",
      "2026-03-01 12:12:47,799 INFO:\tStep 1330 | Train Loss: 0.2458 | Val Loss: 0.5858 | Val F1: 0.5250 | Val P: 0.4179 | Val R: 0.7059\n",
      "2026-03-01 12:12:57,962 INFO:\t[CategoryModel] Epoch 7/12\n",
      "2026-03-01 12:13:47,318 INFO:\tStep 1365 | Train Loss: 0.1761 | Val Loss: 0.6759 | Val F1: 0.5319 | Val P: 0.4601 | Val R: 0.6303\n",
      "2026-03-01 12:14:47,644 INFO:\tStep 1400 | Train Loss: 0.3144 | Val Loss: 0.6602 | Val F1: 0.5536 | Val P: 0.4706 | Val R: 0.6723\n",
      "2026-03-01 12:15:48,535 INFO:\tStep 1435 | Train Loss: 0.2140 | Val Loss: 0.6861 | Val F1: 0.5120 | Val P: 0.3991 | Val R: 0.7143\n",
      "2026-03-01 12:16:48,970 INFO:\tStep 1470 | Train Loss: 0.2140 | Val Loss: 0.6271 | Val F1: 0.4752 | Val P: 0.3447 | Val R: 0.7647\n",
      "2026-03-01 12:17:49,285 INFO:\tStep 1505 | Train Loss: 0.1901 | Val Loss: 0.6841 | Val F1: 0.5343 | Val P: 0.4684 | Val R: 0.6218\n",
      "2026-03-01 12:18:49,519 INFO:\tStep 1540 | Train Loss: 0.1624 | Val Loss: 0.7266 | Val F1: 0.5000 | Val P: 0.4108 | Val R: 0.6387\n",
      "2026-03-01 12:19:16,999 INFO:\t[CategoryModel] Epoch 8/12\n",
      "2026-03-01 12:19:49,014 INFO:\tStep 1575 | Train Loss: 0.1300 | Val Loss: 0.7654 | Val F1: 0.5233 | Val P: 0.4562 | Val R: 0.6134\n",
      "2026-03-01 12:20:49,163 INFO:\tStep 1610 | Train Loss: 0.1756 | Val Loss: 0.8006 | Val F1: 0.5290 | Val P: 0.4293 | Val R: 0.6891\n",
      "2026-03-01 12:21:49,430 INFO:\tStep 1645 | Train Loss: 0.1321 | Val Loss: 0.8834 | Val F1: 0.4956 | Val P: 0.3818 | Val R: 0.7059\n",
      "2026-03-01 12:22:49,780 INFO:\tStep 1680 | Train Loss: 0.1861 | Val Loss: 0.8010 | Val F1: 0.4857 | Val P: 0.3680 | Val R: 0.7143\n",
      "2026-03-01 12:23:50,090 INFO:\tStep 1715 | Train Loss: 0.1035 | Val Loss: 0.8525 | Val F1: 0.5133 | Val P: 0.4254 | Val R: 0.6471\n",
      "2026-03-01 12:24:50,362 INFO:\tStep 1750 | Train Loss: 0.2062 | Val Loss: 0.8395 | Val F1: 0.5217 | Val P: 0.4333 | Val R: 0.6555\n",
      "2026-03-01 12:25:35,197 INFO:\t[CategoryModel] Epoch 9/12\n",
      "2026-03-01 12:25:49,874 INFO:\tStep 1785 | Train Loss: 0.1824 | Val Loss: 0.8047 | Val F1: 0.5226 | Val P: 0.4464 | Val R: 0.6303\n",
      "2026-03-01 12:26:50,042 INFO:\tStep 1820 | Train Loss: 0.1255 | Val Loss: 0.8680 | Val F1: 0.5251 | Val P: 0.4857 | Val R: 0.5714\n",
      "2026-03-01 12:27:50,218 INFO:\tStep 1855 | Train Loss: 0.1033 | Val Loss: 0.9429 | Val F1: 0.5259 | Val P: 0.5000 | Val R: 0.5546\n",
      "2026-03-01 12:28:50,339 INFO:\tStep 1890 | Train Loss: 0.1664 | Val Loss: 0.8858 | Val F1: 0.4943 | Val P: 0.3734 | Val R: 0.7311\n",
      "2026-03-01 12:29:50,713 INFO:\tStep 1925 | Train Loss: 0.1331 | Val Loss: 0.8677 | Val F1: 0.5137 | Val P: 0.4335 | Val R: 0.6303\n",
      "2026-03-01 12:30:50,918 INFO:\tStep 1960 | Train Loss: 0.0971 | Val Loss: 0.8683 | Val F1: 0.5113 | Val P: 0.4158 | Val R: 0.6639\n",
      "2026-03-01 12:31:51,214 INFO:\tStep 1995 | Train Loss: 0.1753 | Val Loss: 0.8570 | Val F1: 0.5128 | Val P: 0.4145 | Val R: 0.6723\n",
      "2026-03-01 12:32:06,490 INFO:\t[CategoryModel] Epoch 10/12\n",
      "2026-03-01 12:32:50,696 INFO:\tStep 2030 | Train Loss: 0.1190 | Val Loss: 0.8525 | Val F1: 0.5347 | Val P: 0.4556 | Val R: 0.6471\n",
      "2026-03-01 12:33:50,919 INFO:\tStep 2065 | Train Loss: 0.1327 | Val Loss: 0.9274 | Val F1: 0.5333 | Val P: 0.5000 | Val R: 0.5714\n",
      "2026-03-01 12:34:51,059 INFO:\tStep 2100 | Train Loss: 0.1374 | Val Loss: 0.8958 | Val F1: 0.5236 | Val P: 0.4615 | Val R: 0.6050\n",
      "2026-03-01 12:35:51,145 INFO:\tStep 2135 | Train Loss: 0.0977 | Val Loss: 0.8996 | Val F1: 0.5113 | Val P: 0.4158 | Val R: 0.6639\n",
      "2026-03-01 12:36:51,458 INFO:\tStep 2170 | Train Loss: 0.0796 | Val Loss: 0.9240 | Val F1: 0.5185 | Val P: 0.4326 | Val R: 0.6471\n",
      "2026-03-01 12:37:51,716 INFO:\tStep 2205 | Train Loss: 0.0949 | Val Loss: 0.9648 | Val F1: 0.5282 | Val P: 0.4545 | Val R: 0.6303\n",
      "2026-03-01 12:38:24,599 INFO:\t[CategoryModel] Epoch 11/12\n",
      "2026-03-01 12:38:51,323 INFO:\tStep 2240 | Train Loss: 0.1232 | Val Loss: 0.9506 | Val F1: 0.5310 | Val P: 0.4503 | Val R: 0.6471\n",
      "2026-03-01 12:38:51,324 INFO:\t[CategoryModel] Early stopping at step 2240\n",
      "2026-03-01 12:39:04,601 INFO:\t[CategoryModel] Optimal threshold: 0.525 (val F1: 0.5594)\n",
      "2026-03-01 12:39:26,690 INFO:\t[CategoryModel] Dev F1: 0.4989 | Dev P: 0.4333 | Dev R: 0.5879\n",
      "2026-03-01 12:39:28,877 INFO:\t[C_multitask] New best saved (val F1=0.5594)\n",
      "\u001b[32m[I 2026-03-01 12:39:29,424]\u001b[0m Trial 0 finished with value: 0.5594405594405595 and parameters: {'lr': 1.102943717573979e-05, 'weight_decay': 0.0071144760093434225, 'hidden_dim': 0, 'head_lr_multiplier': 1, 'category_weight': 0.1}. Best is trial 0 with value: 0.5594405594405595.\u001b[0m\n",
      "2026-03-01 12:39:29,428 INFO:\t[C_multitask] Trial 1: lr=5.53e-05, hidden=0, cat_w=0.5, head_lr_mult=5\n",
      "2026-03-01 12:39:32,141 INFO:\tHTTP Request: HEAD https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-03-01 12:39:32,151 INFO:\tHTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/microsoft/deberta-v3-base/8ccc9b6f36199bec6961081d44eb72fb3f7353f3/config.json \"HTTP/1.1 200 OK\"\n",
      "2026-03-01 12:39:32,251 INFO:\tHTTP Request: HEAD https://huggingface.co/microsoft/deberta-v3-base/resolve/main/model.safetensors \"HTTP/1.1 404 Not Found\"\n",
      "2026-03-01 12:39:32,349 INFO:\tHTTP Request: GET https://huggingface.co/api/models/microsoft/deberta-v3-base \"HTTP/1.1 200 OK\"\n",
      "2026-03-01 12:39:32,464 INFO:\tHTTP Request: GET https://huggingface.co/api/models/microsoft/deberta-v3-base/commits/main \"HTTP/1.1 200 OK\"\n",
      "2026-03-01 12:39:32,581 INFO:\tHTTP Request: GET https://huggingface.co/api/models/microsoft/deberta-v3-base/discussions?p=0 \"HTTP/1.1 200 OK\"\n",
      "2026-03-01 12:39:32,693 INFO:\tHTTP Request: GET https://huggingface.co/api/models/microsoft/deberta-v3-base/commits/refs%2Fpr%2F14 \"HTTP/1.1 200 OK\"\n",
      "2026-03-01 12:39:32,788 INFO:\tHTTP Request: HEAD https://huggingface.co/microsoft/deberta-v3-base/resolve/refs%2Fpr%2F14/model.safetensors.index.json \"HTTP/1.1 404 Not Found\"\n",
      "2026-03-01 12:39:32,883 INFO:\tHTTP Request: HEAD https://huggingface.co/microsoft/deberta-v3-base/resolve/refs%2Fpr%2F14/model.safetensors \"HTTP/1.1 302 Found\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc6f9cb3a36434297be875d49973f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mDebertaV2Model LOAD REPORT\u001b[0m from: microsoft/deberta-v3-base\n",
      "Key                                     | Status     |  | \n",
      "----------------------------------------+------------+--+-\n",
      "lm_predictions.lm_head.bias             | UNEXPECTED |  | \n",
      "mask_predictions.LayerNorm.bias         | UNEXPECTED |  | \n",
      "mask_predictions.LayerNorm.weight       | UNEXPECTED |  | \n",
      "mask_predictions.classifier.bias        | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED |  | \n",
      "mask_predictions.dense.bias             | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED |  | \n",
      "mask_predictions.dense.weight           | UNEXPECTED |  | \n",
      "mask_predictions.classifier.weight      | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "2026-03-01 12:39:34,658 INFO:\tBackbone model loaded: DebertaV2Model, dtype torch.float32, gradient_checkpointing=True\n",
      "2026-03-01 12:39:35,018 INFO:\tpos_weight = 3.09 (raw ratio=9.55, neg=6443, pos=675)\n",
      "2026-03-01 12:39:35,021 INFO:\t[CategoryModel] Early stopping: patience=4 epochs = 24 eval rounds\n",
      "2026-03-01 12:39:35,022 INFO:\t[CategoryModel] Epoch 1/12\n",
      "2026-03-01 12:40:33,737 INFO:\tStep 35 | Train Loss: 1.0020 | Val Loss: 0.6646 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 12:41:32,585 INFO:\tStep 70 | Train Loss: 0.7623 | Val Loss: 0.7209 | Val F1: 0.0000 | Val P: 0.0000 | Val R: 0.0000\n",
      "2026-03-01 12:42:31,506 INFO:\tStep 105 | Train Loss: 0.6766 | Val Loss: 0.5121 | Val F1: 0.4180 | Val P: 0.3385 | Val R: 0.5462\n",
      "2026-03-01 12:43:30,791 INFO:\tStep 140 | Train Loss: 0.7168 | Val Loss: 0.5451 | Val F1: 0.3114 | Val P: 0.5417 | Val R: 0.2185\n",
      "2026-03-01 12:44:29,509 INFO:\tStep 175 | Train Loss: 0.5927 | Val Loss: 0.5181 | Val F1: 0.4072 | Val P: 0.2786 | Val R: 0.7563\n",
      "2026-03-01 12:45:28,319 INFO:\tStep 210 | Train Loss: 0.6711 | Val Loss: 0.5849 | Val F1: 0.3509 | Val P: 0.2217 | Val R: 0.8403\n",
      "2026-03-01 12:45:44,751 INFO:\t[CategoryModel] Epoch 2/12\n",
      "2026-03-01 12:46:26,458 INFO:\tStep 245 | Train Loss: 0.5797 | Val Loss: 0.5046 | Val F1: 0.4198 | Val P: 0.2918 | Val R: 0.7479\n",
      "2026-03-01 12:47:26,133 INFO:\tStep 280 | Train Loss: 0.5463 | Val Loss: 0.7125 | Val F1: 0.3814 | Val P: 0.4933 | Val R: 0.3109\n",
      "2026-03-01 12:48:24,878 INFO:\tStep 315 | Train Loss: 0.7566 | Val Loss: 0.5937 | Val F1: 0.3250 | Val P: 0.6341 | Val R: 0.2185\n",
      "2026-03-01 12:49:23,619 INFO:\tStep 350 | Train Loss: 0.6024 | Val Loss: 0.5261 | Val F1: 0.4000 | Val P: 0.2659 | Val R: 0.8067\n",
      "2026-03-01 12:50:22,229 INFO:\tStep 385 | Train Loss: 0.7053 | Val Loss: 0.5589 | Val F1: 0.4291 | Val P: 0.4141 | Val R: 0.4454\n",
      "2026-03-01 12:51:21,436 INFO:\tStep 420 | Train Loss: 0.5174 | Val Loss: 0.5579 | Val F1: 0.4414 | Val P: 0.4757 | Val R: 0.4118\n",
      "2026-03-01 12:51:55,553 INFO:\t[CategoryModel] Epoch 3/12\n",
      "2026-03-01 12:52:20,333 INFO:\tStep 455 | Train Loss: 0.6027 | Val Loss: 0.4831 | Val F1: 0.4471 | Val P: 0.3439 | Val R: 0.6387\n",
      "2026-03-01 12:53:19,632 INFO:\tStep 490 | Train Loss: 0.6673 | Val Loss: 0.6609 | Val F1: 0.4254 | Val P: 0.3826 | Val R: 0.4790\n",
      "2026-03-01 12:54:18,381 INFO:\tStep 525 | Train Loss: 0.8121 | Val Loss: 0.6312 | Val F1: 0.2924 | Val P: 0.4808 | Val R: 0.2101\n",
      "2026-03-01 12:55:20,538 INFO:\tStep 560 | Train Loss: 0.5444 | Val Loss: 0.7858 | Val F1: 0.3106 | Val P: 0.5952 | Val R: 0.2101\n",
      "2026-03-01 12:56:19,449 INFO:\tStep 595 | Train Loss: 0.8066 | Val Loss: 0.5936 | Val F1: 0.4195 | Val P: 0.5000 | Val R: 0.3613\n",
      "2026-03-01 12:57:12,838 INFO:\tStep 630 | Train Loss: 0.6175 | Val Loss: 0.5938 | Val F1: 0.4331 | Val P: 0.4074 | Val R: 0.4622\n",
      "2026-03-01 12:58:04,445 INFO:\tStep 665 | Train Loss: 0.5649 | Val Loss: 0.5820 | Val F1: 0.4519 | Val P: 0.4500 | Val R: 0.4538\n",
      "2026-03-01 12:58:08,976 INFO:\t[CategoryModel] Epoch 4/12\n",
      "2026-03-01 12:58:56,452 INFO:\tStep 700 | Train Loss: 0.5773 | Val Loss: 0.5508 | Val F1: 0.4502 | Val P: 0.4643 | Val R: 0.4370\n",
      "2026-03-01 12:59:47,381 INFO:\tStep 735 | Train Loss: 0.5112 | Val Loss: 0.5962 | Val F1: 0.4551 | Val P: 0.3679 | Val R: 0.5966\n",
      "2026-03-01 13:00:38,506 INFO:\tStep 770 | Train Loss: 0.4927 | Val Loss: 0.6356 | Val F1: 0.4233 | Val P: 0.5714 | Val R: 0.3361\n",
      "2026-03-01 13:01:33,223 INFO:\tStep 805 | Train Loss: 0.6348 | Val Loss: 0.6884 | Val F1: 0.2914 | Val P: 0.6875 | Val R: 0.1849\n",
      "2026-03-01 13:02:26,941 INFO:\tStep 840 | Train Loss: 0.7363 | Val Loss: 0.6176 | Val F1: 0.1549 | Val P: 0.4783 | Val R: 0.0924\n",
      "2026-03-01 13:03:22,411 INFO:\tStep 875 | Train Loss: 0.5835 | Val Loss: 0.6159 | Val F1: 0.3828 | Val P: 0.2676 | Val R: 0.6723\n",
      "2026-03-01 13:03:43,596 INFO:\t[CategoryModel] Epoch 5/12\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=f\"pcl_deberta_exp_{EXP_NAME}\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=SEED),\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=6, n_warmup_steps=300),\n",
    ")\n",
    "study.optimize(objective, n_trials=N_TRIALS)\n",
    "\n",
    "best = study.best_trial\n",
    "LOG.info(f\"Best trial: {best.number}\")\n",
    "LOG.info(f\"Val F1: {best.user_attrs['best_val_f1']:.4f} | Dev F1: {best.user_attrs['dev_f1']:.4f}\")\n",
    "LOG.info(f\"Best params: {best.params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0000009",
   "metadata": {},
   "source": [
    "## 4. Results\n",
    "\n",
    "Binary head dev metrics + per-category predictions from the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0000010",
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot_fn, suffix in [\n",
    "    (plot_optimization_history, \"history\"),\n",
    "    (plot_param_importances, \"importances\"),\n",
    "    (plot_parallel_coordinate, \"parallel\"),\n",
    "]:\n",
    "    plot_fn(study)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUT_DIR}/{EXP_NAME}_optuna_{suffix}.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "best = study.best_trial\n",
    "best_params = best.params\n",
    "\n",
    "model = PCLDeBERTa(\n",
    "    hidden_dim=best_params[\"hidden_dim\"],\n",
    "    dropout_rate=best_params.get(\"dropout_rate\", 0.0),\n",
    "    pooling=POOLING,\n",
    "    n_out=1 + N_CATEGORIES,\n",
    ").to(DEVICE)\n",
    "\n",
    "state_dict = torch.load(\n",
    "    os.path.join(OUT_DIR, f\"exp_{EXP_NAME}_best_model.pt\"), map_location=DEVICE\n",
    ")\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "_, _, dev_loader = make_category_dataloaders(\n",
    "    train_sub_df, val_sub_df, dev_df, BATCH_SIZE, MAX_LENGTH, tokeniser\n",
    ")\n",
    "\n",
    "best_threshold = best.user_attrs[\"best_threshold\"]\n",
    "model.eval()\n",
    "all_binary_scores, all_binary_labels = [], []\n",
    "all_cat_probs, all_cat_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dev_loader:\n",
    "        input_ids      = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        all_binary_scores.append(logits[:, 0].cpu())\n",
    "        all_binary_labels.append(batch[\"labels\"].cpu())\n",
    "        all_cat_probs.append(torch.sigmoid(logits[:, 1:]).cpu())\n",
    "        all_cat_labels.append(batch[\"category_labels\"].cpu())\n",
    "\n",
    "binary_probs  = torch.sigmoid(torch.cat(all_binary_scores))\n",
    "binary_labels = torch.cat(all_binary_labels).long().numpy()\n",
    "binary_preds  = (binary_probs >= best_threshold).long().numpy()\n",
    "cat_probs  = torch.cat(all_cat_probs).numpy()   # (N, 7)\n",
    "cat_labels = torch.cat(all_cat_labels).numpy()  # (N, 7)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"{EXP_NAME.upper()} — Dev Set Binary Results (threshold={best_threshold:.3f})\")\n",
    "print(f\"{'='*60}\")\n",
    "print(classification_report(binary_labels, binary_preds, target_names=[\"Non-PCL\", \"PCL\"]))\n",
    "\n",
    "from sklearn.metrics import f1_score as sk_f1\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Per-category dev F1 (threshold=0.5):\")\n",
    "print(f\"{'='*60}\")\n",
    "for i, cat in enumerate(PCL_CATEGORIES):\n",
    "    cat_pred = (cat_probs[:, i] >= 0.5).astype(int)\n",
    "    f1 = sk_f1(cat_labels[:, i].astype(int), cat_pred, zero_division=0)\n",
    "    n_pos = int(cat_labels[:, i].sum())\n",
    "    print(f\"  {cat:<35s} F1={f1:.4f}  (dev positives: {n_pos})\")\n",
    "\n",
    "print(\"\\nBest hyperparams:\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(f\"  pooling: mean (fixed)\")\n",
    "print(f\"  warmup_fraction: 0.10 (fixed)\")\n",
    "print(f\"  label_smoothing: 0.0 (fixed)\")\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85e858d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfbea65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
