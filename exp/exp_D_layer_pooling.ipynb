{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0000001",
   "metadata": {},
   "source": [
    "# Experiment D: Scalar Mix Layer Pooling\n",
    "\n",
    "Adds `PoolingStrategy.SCALAR_MIX` to the pooling search space: a learned convex\n",
    "combination of all 13 DeBERTa hidden states (embedding layer + 12 transformer layers),\n",
    "followed by attention-mask-weighted mean pooling.\n",
    "\n",
    "Rationale: DeBERTa lower layers encode syntax, upper layers encode semantics;\n",
    "PCL detection may benefit from intermediate representations.\n",
    "\n",
    "No extra features (isolate the effect of pooling).\n",
    "Fixed corrections: VAL_FRACTION=0.15, BATCH_SIZE=32, NUM_EPOCHS=12, PATIENCE=4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import logging\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.metrics import classification_report\n",
    "import optuna\n",
    "from optuna.visualization.matplotlib import (\n",
    "    plot_optimization_history,\n",
    "    plot_param_importances,\n",
    "    plot_parallel_coordinate,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "from utils.data import load_data\n",
    "from utils.split import split_train_val\n",
    "from utils.dataloaders import make_dataloaders\n",
    "from utils.pcl_deberta import PCLDeBERTa, PoolingStrategy\n",
    "from utils.optim import compute_pos_weight\n",
    "from utils.training_loop import train_model\n",
    "from utils.eval import evaluate\n",
    "\n",
    "SEED = 42\n",
    "DATA_DIR = \"../data\"\n",
    "OUT_DIR = \"out\"\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "MAX_LENGTH = 256\n",
    "VAL_FRACTION = 0.15\n",
    "BATCH_SIZE = 32\n",
    "N_TRIALS = 20\n",
    "NUM_EPOCHS = 12\n",
    "PATIENCE = 4\n",
    "N_EVAL_STEPS = 35\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s:\\t%(message)s\")\n",
    "LOG = logging.getLogger(__name__)\n",
    "LOG.info(f\"Device: {DEVICE}\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0000003",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, dev_df = load_data(DATA_DIR)\n",
    "train_sub_df, val_sub_df = split_train_val(train_df, val_frac=VAL_FRACTION, seed=SEED)\n",
    "tokeniser = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "LOG.info(f\"Train: {len(train_sub_df)}, Val: {len(val_sub_df)}, Dev: {len(dev_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0000005",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter Search\n",
    "\n",
    "The pooling search space is extended to include `scalar_mix` alongside the standard four strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0000006",
   "metadata": {},
   "outputs": [],
   "source": [
    "POOLING_MAP = {\n",
    "    \"cls\": PoolingStrategy.CLS,\n",
    "    \"mean\": PoolingStrategy.MEAN,\n",
    "    \"max\": PoolingStrategy.MAX,\n",
    "    \"cls_mean\": PoolingStrategy.CLS_MEAN,\n",
    "    \"scalar_mix\": PoolingStrategy.SCALAR_MIX,\n",
    "}\n",
    "EXP_NAME = \"D_layer_pooling\"\n",
    "\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    lr              = trial.suggest_float(\"lr\", 4e-6, 6e-5, log=True)\n",
    "    warmup_fraction = trial.suggest_float(\"warmup_fraction\", 0.03, 0.20, step=0.01)\n",
    "    hidden_dim      = trial.suggest_categorical(\"hidden_dim\", [0, 128, 256, 512])\n",
    "    dropout_rate    = trial.suggest_float(\"dropout_rate\", 0.0, 0.4, step=0.05) if hidden_dim > 0 else 0.0\n",
    "    weight_decay    = trial.suggest_float(\"weight_decay\", 1e-5, 1e-2, log=True)\n",
    "    head_lr_mult    = trial.suggest_categorical(\"head_lr_multiplier\", [1, 3, 5, 10])\n",
    "    label_smoothing = trial.suggest_float(\"label_smoothing\", 0.0, 0.15, step=0.025)\n",
    "    pooling_name    = trial.suggest_categorical(\n",
    "        \"pooling\", [\"cls\", \"mean\", \"max\", \"cls_mean\", \"scalar_mix\"]\n",
    "    )\n",
    "\n",
    "    pooling = POOLING_MAP[pooling_name]\n",
    "\n",
    "    LOG.info(f\"[{EXP_NAME}] Trial {trial.number}: lr={lr:.2e}, pool={pooling_name}, hidden={hidden_dim}\")\n",
    "\n",
    "    train_loader, val_loader, dev_loader = make_dataloaders(\n",
    "        train_sub_df, val_sub_df, dev_df, BATCH_SIZE, MAX_LENGTH, tokeniser\n",
    "    )\n",
    "\n",
    "    model = PCLDeBERTa(\n",
    "        hidden_dim=hidden_dim, dropout_rate=dropout_rate, pooling=pooling\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    pos_weight = compute_pos_weight(train_sub_df, DEVICE)\n",
    "\n",
    "    results = train_model(\n",
    "        model=model, device=DEVICE,\n",
    "        train_loader=train_loader, val_loader=val_loader, dev_loader=dev_loader,\n",
    "        pos_weight=pos_weight, lr=lr, weight_decay=weight_decay,\n",
    "        num_epochs=NUM_EPOCHS, warmup_fraction=warmup_fraction,\n",
    "        patience=PATIENCE, head_lr_multiplier=head_lr_mult,\n",
    "        label_smoothing=label_smoothing, eval_every_n_steps=N_EVAL_STEPS,\n",
    "        trial=trial,\n",
    "    )\n",
    "\n",
    "    trial.set_user_attr(\"best_val_f1\",    results[\"best_val_f1\"])\n",
    "    trial.set_user_attr(\"best_threshold\", results[\"best_threshold\"])\n",
    "    trial.set_user_attr(\"dev_f1\",         results[\"dev_metrics\"][\"f1\"])\n",
    "    trial.set_user_attr(\"dev_precision\",  results[\"dev_metrics\"][\"precision\"])\n",
    "    trial.set_user_attr(\"dev_recall\",     results[\"dev_metrics\"][\"recall\"])\n",
    "\n",
    "    try:\n",
    "        prev_best = trial.study.best_value\n",
    "    except ValueError:\n",
    "        prev_best = -float(\"inf\")\n",
    "    if results[\"best_val_f1\"] > prev_best:\n",
    "        torch.save(\n",
    "            {k: v.cpu() for k, v in model.state_dict().items()},\n",
    "            os.path.join(OUT_DIR, f\"exp_{EXP_NAME}_best_model.pt\")\n",
    "        )\n",
    "        config = {**trial.params, \"batch_size\": BATCH_SIZE, \"num_epochs\": NUM_EPOCHS,\n",
    "                  \"patience\": PATIENCE, \"best_threshold\": results[\"best_threshold\"]}\n",
    "        with open(os.path.join(OUT_DIR, f\"exp_{EXP_NAME}_best_params.json\"), \"w\") as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        LOG.info(f\"[{EXP_NAME}] New best saved (val F1={results['best_val_f1']:.4f})\")\n",
    "\n",
    "    del model, train_loader, val_loader, dev_loader\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return results[\"best_val_f1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0000007",
   "metadata": {},
   "source": [
    "## 3. Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0000008",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=f\"pcl_deberta_exp_{EXP_NAME}\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=SEED),\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=6, n_warmup_steps=300),\n",
    ")\n",
    "study.optimize(objective, n_trials=N_TRIALS)\n",
    "\n",
    "best = study.best_trial\n",
    "LOG.info(f\"Best trial: {best.number}\")\n",
    "LOG.info(f\"Val F1: {best.user_attrs['best_val_f1']:.4f} | Dev F1: {best.user_attrs['dev_f1']:.4f}\")\n",
    "LOG.info(f\"Best params: {best.params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0000009",
   "metadata": {},
   "source": [
    "## 4. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0000010",
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot_fn, suffix in [\n",
    "    (plot_optimization_history, \"history\"),\n",
    "    (plot_param_importances, \"importances\"),\n",
    "    (plot_parallel_coordinate, \"parallel\"),\n",
    "]:\n",
    "    plot_fn(study)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUT_DIR}/{EXP_NAME}_optuna_{suffix}.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "best = study.best_trial\n",
    "best_params = best.params\n",
    "pooling = POOLING_MAP[best_params[\"pooling\"]]\n",
    "\n",
    "model = PCLDeBERTa(\n",
    "    hidden_dim=best_params[\"hidden_dim\"],\n",
    "    dropout_rate=best_params.get(\"dropout_rate\", 0.0),\n",
    "    pooling=pooling,\n",
    ").to(DEVICE)\n",
    "\n",
    "state_dict = torch.load(\n",
    "    os.path.join(OUT_DIR, f\"exp_{EXP_NAME}_best_model.pt\"), map_location=DEVICE\n",
    ")\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "_, _, dev_loader = make_dataloaders(train_sub_df, val_sub_df, dev_df, BATCH_SIZE, MAX_LENGTH, tokeniser)\n",
    "dev_metrics = evaluate(model, DEVICE, dev_loader, threshold=best.user_attrs[\"best_threshold\"])\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"{EXP_NAME.upper()} â€” Dev Set Results (threshold={best.user_attrs['best_threshold']:.3f})\")\n",
    "print(f\"{'='*60}\")\n",
    "print(classification_report(dev_metrics[\"labels\"], dev_metrics[\"preds\"], target_names=[\"Non-PCL\", \"PCL\"]))\n",
    "for k, v in best_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Inspect learned layer weights if scalar_mix was selected\n",
    "if best_params[\"pooling\"] == \"scalar_mix\":\n",
    "    weights = torch.softmax(model.layer_weights, dim=0).detach().cpu().numpy()\n",
    "    print(\"\\nLearned layer weights (embedding=0, transformer=1..12):\")\n",
    "    for i, w in enumerate(weights):\n",
    "        print(f\"  Layer {i:2d}: {w:.4f}\")\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ]
}
