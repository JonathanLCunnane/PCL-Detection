{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0000001",
   "metadata": {},
   "source": [
    "# Experiment B: Focal Loss\n",
    "\n",
    "Replace `BCEWithLogitsLoss` with `FocalLoss` (gamma searched as hyperparameter).\n",
    "Focal loss down-weights trivially easy negatives, concentrating gradient on\n",
    "hard/borderline cases — particularly useful at the ~10:1 class imbalance here.\n",
    "\n",
    "No extra features; pure DeBERTa fine-tuning with fixed hyperparameter corrections:\n",
    "VAL_FRACTION=0.15, BATCH_SIZE=32, NUM_EPOCHS=12, PATIENCE=4, pooling searched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import logging\n",
    "import gc\n",
    "import json\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "import optuna\n",
    "from optuna.visualization.matplotlib import (\n",
    "    plot_optimization_history,\n",
    "    plot_param_importances,\n",
    "    plot_parallel_coordinate,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "from utils.data import load_data\n",
    "from utils.split import split_train_val\n",
    "from utils.dataloaders import make_dataloaders\n",
    "from utils.pcl_deberta import PCLDeBERTa, PoolingStrategy\n",
    "from utils.losses import FocalLoss\n",
    "from utils.optim import get_cosine_schedule_with_warmup, compute_pos_weight\n",
    "from utils.early_stopping import EarlyStopping\n",
    "from utils.eval import evaluate, find_best_threshold\n",
    "\n",
    "SEED = 42\n",
    "DATA_DIR = \"../data\"\n",
    "OUT_DIR = \"out\"\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "MAX_LENGTH = 256\n",
    "VAL_FRACTION = 0.15\n",
    "BATCH_SIZE = 32\n",
    "N_TRIALS = 20\n",
    "NUM_EPOCHS = 12\n",
    "PATIENCE = 4\n",
    "N_EVAL_STEPS = 35\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s:\\t%(message)s\")\n",
    "LOG = logging.getLogger(__name__)\n",
    "LOG.info(f\"Device: {DEVICE}\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0000003",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, dev_df = load_data(DATA_DIR)\n",
    "train_sub_df, val_sub_df = split_train_val(train_df, val_frac=VAL_FRACTION, seed=SEED)\n",
    "tokeniser = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "LOG.info(f\"Train: {len(train_sub_df)}, Val: {len(val_sub_df)}, Dev: {len(dev_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0000005",
   "metadata": {},
   "source": [
    "## 2. Focal-Loss Training Function\n",
    "\n",
    "Same as `train_model` but uses `FocalLoss(gamma, pos_weight)` instead of `BCEWithLogitsLoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0000006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_focal(\n",
    "    model, device, train_loader, val_loader, dev_loader,\n",
    "    pos_weight, lr, weight_decay, num_epochs, warmup_fraction,\n",
    "    patience, gamma, head_lr_multiplier=3.0, label_smoothing=0.0,\n",
    "    eval_every_n_steps=50, trial=None,\n",
    ") -> dict:\n",
    "    \"\"\"train_model variant using FocalLoss instead of BCEWithLogitsLoss.\"\"\"\n",
    "    criterion = FocalLoss(gamma=gamma, pos_weight=pos_weight)\n",
    "\n",
    "    backbone_params = list(model.backbone.parameters())\n",
    "    head_params = list(model.classifier.parameters())\n",
    "    optimizer = AdamW([\n",
    "        {\"params\": backbone_params, \"lr\": lr},\n",
    "        {\"params\": head_params, \"lr\": lr * head_lr_multiplier},\n",
    "    ], weight_decay=weight_decay)\n",
    "\n",
    "    total_steps = len(train_loader) * num_epochs\n",
    "    warmup_steps = int(total_steps * warmup_fraction)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "    evals_per_epoch = max(1, len(train_loader) // eval_every_n_steps)\n",
    "    patience_in_evals = patience * evals_per_epoch\n",
    "    early_stopper = EarlyStopping(patience=patience_in_evals)\n",
    "\n",
    "    model.train()\n",
    "    global_step = 0\n",
    "    train_losses = []\n",
    "    best_val_f1 = 0.0\n",
    "    best_state_dict = None\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        LOG.info(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        for batch in train_loader:\n",
    "            input_ids      = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels         = batch[\"labels\"].to(device)\n",
    "\n",
    "            if label_smoothing > 0:\n",
    "                labels = labels * (1 - label_smoothing) + 0.5 * label_smoothing\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            scores = model(input_ids=input_ids, attention_mask=attention_mask).squeeze(-1)\n",
    "            loss = criterion(scores, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_every_n_steps == 0:\n",
    "                avg_loss = running_loss / eval_every_n_steps\n",
    "                train_losses.append(avg_loss)\n",
    "                running_loss = 0.0\n",
    "\n",
    "                val_metrics = evaluate(model, device, val_loader, criterion=criterion)\n",
    "                val_f1 = val_metrics[\"f1\"]\n",
    "                LOG.info(f\"Step {global_step} | Loss: {avg_loss:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "                if val_f1 > best_val_f1:\n",
    "                    best_val_f1 = val_f1\n",
    "                    best_state_dict = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "                if trial is not None:\n",
    "                    trial.report(val_f1, global_step)\n",
    "                    if trial.should_prune():\n",
    "                        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "                if early_stopper.step(val_f1):\n",
    "                    LOG.info(f\"Early stopping at step {global_step}\")\n",
    "                    break\n",
    "\n",
    "        if early_stopper.should_stop:\n",
    "            break\n",
    "\n",
    "    if best_state_dict is not None:\n",
    "        model.load_state_dict(best_state_dict)\n",
    "\n",
    "    best_thresh, thresh_val_f1 = find_best_threshold(model, device, val_loader)\n",
    "    dev_metrics = evaluate(model, device, dev_loader, criterion=criterion, threshold=best_thresh)\n",
    "    LOG.info(f\"Threshold: {best_thresh:.3f} | Dev F1: {dev_metrics['f1']:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"best_val_f1\": thresh_val_f1,\n",
    "        \"best_threshold\": best_thresh,\n",
    "        \"dev_metrics\": dev_metrics,\n",
    "        \"train_losses\": train_losses,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0000007",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0000008",
   "metadata": {},
   "outputs": [],
   "source": [
    "POOLING_MAP = {\n",
    "    \"cls\": PoolingStrategy.CLS,\n",
    "    \"mean\": PoolingStrategy.MEAN,\n",
    "    \"max\": PoolingStrategy.MAX,\n",
    "    \"cls_mean\": PoolingStrategy.CLS_MEAN,\n",
    "}\n",
    "EXP_NAME = \"B_focal\"\n",
    "\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    lr              = trial.suggest_float(\"lr\", 4e-6, 6e-5, log=True)\n",
    "    warmup_fraction = trial.suggest_float(\"warmup_fraction\", 0.03, 0.20, step=0.01)\n",
    "    hidden_dim      = trial.suggest_categorical(\"hidden_dim\", [0, 128, 256, 512])\n",
    "    dropout_rate    = trial.suggest_float(\"dropout_rate\", 0.0, 0.4, step=0.05) if hidden_dim > 0 else 0.0\n",
    "    weight_decay    = trial.suggest_float(\"weight_decay\", 1e-5, 1e-2, log=True)\n",
    "    head_lr_mult    = trial.suggest_categorical(\"head_lr_multiplier\", [1, 3, 5, 10])\n",
    "    label_smoothing = trial.suggest_float(\"label_smoothing\", 0.0, 0.15, step=0.025)\n",
    "    pooling_name    = trial.suggest_categorical(\"pooling\", [\"cls\", \"mean\", \"max\", \"cls_mean\"])\n",
    "    gamma           = trial.suggest_categorical(\"gamma\", [0.5, 1.0, 1.5, 2.0, 2.5, 3.0])\n",
    "\n",
    "    pooling = POOLING_MAP[pooling_name]\n",
    "\n",
    "    LOG.info(f\"[{EXP_NAME}] Trial {trial.number}: lr={lr:.2e}, gamma={gamma}, pool={pooling_name}\")\n",
    "\n",
    "    train_loader, val_loader, dev_loader = make_dataloaders(\n",
    "        train_sub_df, val_sub_df, dev_df, BATCH_SIZE, MAX_LENGTH, tokeniser\n",
    "    )\n",
    "\n",
    "    model = PCLDeBERTa(\n",
    "        hidden_dim=hidden_dim, dropout_rate=dropout_rate, pooling=pooling\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    pos_weight = compute_pos_weight(train_sub_df, DEVICE)\n",
    "\n",
    "    results = train_model_focal(\n",
    "        model=model, device=DEVICE,\n",
    "        train_loader=train_loader, val_loader=val_loader, dev_loader=dev_loader,\n",
    "        pos_weight=pos_weight, lr=lr, weight_decay=weight_decay,\n",
    "        num_epochs=NUM_EPOCHS, warmup_fraction=warmup_fraction,\n",
    "        patience=PATIENCE, gamma=gamma,\n",
    "        head_lr_multiplier=head_lr_mult,\n",
    "        label_smoothing=label_smoothing,\n",
    "        eval_every_n_steps=N_EVAL_STEPS,\n",
    "        trial=trial,\n",
    "    )\n",
    "\n",
    "    trial.set_user_attr(\"best_val_f1\",    results[\"best_val_f1\"])\n",
    "    trial.set_user_attr(\"best_threshold\", results[\"best_threshold\"])\n",
    "    trial.set_user_attr(\"dev_f1\",         results[\"dev_metrics\"][\"f1\"])\n",
    "    trial.set_user_attr(\"dev_precision\",  results[\"dev_metrics\"][\"precision\"])\n",
    "    trial.set_user_attr(\"dev_recall\",     results[\"dev_metrics\"][\"recall\"])\n",
    "\n",
    "    try:\n",
    "        prev_best = trial.study.best_value\n",
    "    except ValueError:\n",
    "        prev_best = -float(\"inf\")\n",
    "    if results[\"best_val_f1\"] > prev_best:\n",
    "        torch.save(\n",
    "            {k: v.cpu() for k, v in model.state_dict().items()},\n",
    "            os.path.join(OUT_DIR, f\"exp_{EXP_NAME}_best_model.pt\")\n",
    "        )\n",
    "        config = {**trial.params, \"batch_size\": BATCH_SIZE, \"num_epochs\": NUM_EPOCHS,\n",
    "                  \"patience\": PATIENCE, \"best_threshold\": results[\"best_threshold\"]}\n",
    "        with open(os.path.join(OUT_DIR, f\"exp_{EXP_NAME}_best_params.json\"), \"w\") as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        LOG.info(f\"[{EXP_NAME}] New best saved (val F1={results['best_val_f1']:.4f})\")\n",
    "\n",
    "    del model, train_loader, val_loader, dev_loader\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return results[\"best_val_f1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0000009",
   "metadata": {},
   "source": [
    "## 4. Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0000010",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=f\"pcl_deberta_exp_{EXP_NAME}\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=SEED),\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=6, n_warmup_steps=300),\n",
    ")\n",
    "study.optimize(objective, n_trials=N_TRIALS)\n",
    "\n",
    "best = study.best_trial\n",
    "LOG.info(f\"Best trial: {best.number}\")\n",
    "LOG.info(f\"Val F1: {best.user_attrs['best_val_f1']:.4f} | Dev F1: {best.user_attrs['dev_f1']:.4f}\")\n",
    "LOG.info(f\"Best params: {best.params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0000011",
   "metadata": {},
   "source": [
    "## 5. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0000012",
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot_fn, suffix in [\n",
    "    (plot_optimization_history, \"history\"),\n",
    "    (plot_param_importances, \"importances\"),\n",
    "    (plot_parallel_coordinate, \"parallel\"),\n",
    "]:\n",
    "    plot_fn(study)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUT_DIR}/{EXP_NAME}_optuna_{suffix}.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "best = study.best_trial\n",
    "best_params = best.params\n",
    "pooling = POOLING_MAP[best_params[\"pooling\"]]\n",
    "\n",
    "model = PCLDeBERTa(\n",
    "    hidden_dim=best_params[\"hidden_dim\"],\n",
    "    dropout_rate=best_params.get(\"dropout_rate\", 0.0),\n",
    "    pooling=pooling,\n",
    ").to(DEVICE)\n",
    "\n",
    "state_dict = torch.load(\n",
    "    os.path.join(OUT_DIR, f\"exp_{EXP_NAME}_best_model.pt\"), map_location=DEVICE\n",
    ")\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "_, _, dev_loader = make_dataloaders(train_sub_df, val_sub_df, dev_df, BATCH_SIZE, MAX_LENGTH, tokeniser)\n",
    "pos_weight = compute_pos_weight(train_sub_df, DEVICE)\n",
    "focal = FocalLoss(gamma=best_params[\"gamma\"], pos_weight=pos_weight)\n",
    "dev_metrics = evaluate(model, DEVICE, dev_loader, criterion=focal,\n",
    "                       threshold=best.user_attrs[\"best_threshold\"])\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"{EXP_NAME.upper()} — Dev Set Results (threshold={best.user_attrs['best_threshold']:.3f})\")\n",
    "print(f\"{'='*60}\")\n",
    "print(classification_report(dev_metrics[\"labels\"], dev_metrics[\"preds\"], target_names=[\"Non-PCL\", \"PCL\"]))\n",
    "for k, v in best_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ]
}
